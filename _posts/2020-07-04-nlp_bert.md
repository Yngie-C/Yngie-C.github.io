---
layout: post
title: BERT (Bidirectional Encoder Representations from Transformer)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# BERT

<p align="center"><img src="https://www.sesameworkshop.org/sites/default/files/imageservicecache/2019-02/presskit_ss_bio_bert.png/f17cd1b4d342d2ce36145b3a9d6e2e1a.jpg" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://www.sesameworkshop.org/press-room/press-kits/sesame-street-season-50/people-your-neighborhood-ss50#bert%C2%A0">sesameworkshop.org</a></p>

**BERT(Bidirectional Encoder Representations from Transformer)**는 구글이 2018년 10월에 [*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805) 라는 논문을 통해서 발표한 모델입니다. 모델 명에서부터 트랜스포머 인코더 기반의 구조를 사용했음을 알 수 있습니다.

BERT의 Pre-training은 2가지 방법으로 진행됩니다. 각각 Masked LM과 NSP입니다. 각각의 방법에 대해서는 아래에서 다시 설명하겠습니다. 대량의 데이터를 Pre-training한 이후에는 모델을 사용하고자 하는 태스크에 맞게 Fine-tuning하는 과정이 필요합니다. 아래는 대량의 말뭉치를 통해 BERT를 MLM과 NSP로 Pre-training한 후에 태스크에 맞게(Task specific) Fine-tuning하는 과정을 보여주는 이미지입니다.

<p align="center"><img src="https://www.researchgate.net/profile/Jan_Christian_Blaise_Cruz/publication/334160936/figure/fig1/AS:776030256111617@1562031439583/Overall-BERT-pretraining-and-finetuning-framework-Note-that-the-same-architecture-in.ppm" alt="bert_pre_fine"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

## Structure & Data

논문에서 발표한 BERT 모델은 BASE 모델과 LARGE 모델이 있습니다.

<p align="center"><img src="http://jalammar.github.io/images/bert-base-bert-large.png" alt="bert2" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-bert/">jalammar.github.io</a></p>

두 모델 모두 트랜스포머의 인코더 블록을 중첩하여 사용합니다. 논문에서 사용한 BERT BASE 모델은 총 12개의 인코더 블록으로 이루어져 있습니다. 임베딩 벡터의 차원은 768을 사용하였고 12개의 Multi Head-Attention을 사용하였습니다. 파라미터의 개수는 1억 1천만(110M)개이며 이전에 발표되었던 [GPT](https://yngie-c.github.io/nlp/2020/07/05/nlp_gpt/)와의 성능 비교를 용이하게 하기 위하여 이렇게 설정한 것으로 보입니다.

사이즈를 더 키운 BERT LARGE 모델은 24개의 인코더 블록을 중첩하여 구성하였습니다. 임베딩 벡터의 차원도 1024로 늘어났으며 16개의 Multi Head-Attention을 사용하였습니다. LARGE 모델은 총 3억 4천만(340M)개의 파라미터를 사용합니다. 아래는 각 모델에 사용된 인코더 블록의 개수를 나타내는 이미지 입니다.

<p align="center"><img src="http://jalammar.github.io/images/bert-base-bert-large-encoders.png" alt="bert3" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-bert/">jalammar.github.io</a></p>

아래 그림은 BERT와 GPT, ELMo의 구조를 도식화하여 비교한 것입니다. 가장 오른쪽에 있는 ELMo는 양방향 LSTM을 사용하여 학습시킨 후에 생성되는 은닉 벡터를 선형 결합하여 임베딩 벡터로 사용하는 것을 잘 보여주고 있습니다. 가운데 있는 GPT는 디코더 블록을 쌓아 설계한 모델이기 때문에 Masked Self-Attention의 영향을 받아 앞쪽에 위치한 단어의 영향만을 받는다는 것을 잘 보여주고 있지요. 마지막으로 BERT는 양방향으로 모든 단어를 동시에 보면서 학습함을 나타내고 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514876-0ba14180-be50-11ea-9153-36a8120244ed.png" alt="bert_str"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

BERT의 학습에는 약 8억(800M) 단어로 구성된 BookCorpus와 25억 단어로 구성된 Wikipedia 말뭉치를 사용하였습니다. 한 번에 입력할 수 있는 최대 토큰 길이는 512개이며 배치 사이즈는 256을 사용하였습니다. 옵티마이저(Optimizer)로는 Adam을  $1.0 \times 10^{-4}$의 학습률(Learning rate)로 사용하는 것을 볼 수 있습니다. 모든 층마다 0.1의 드롭아웃(Dropout)이 적용되어 있으며 활성화 함수(Activation function)로는 GeLU를 사용하였습니다. 

## Input 'Sentence'

BERT는 입력 데이터를 유연하게 설정할 수 있도록 설계되었습니다. 이 덕분에 입력 데이터를 하나의 Sentence로만 구성할 수도 있고 한 쌍의 Sentence로 구성할 수도 있습니다. 그러나 주의할 점이 있습니다. BERT에서 사용하는 *"Sentence"*의 의미는 일반적인 의미로 사용되는 *"문장"*과 다르다는 점입니다. 그렇기 때문에 여기서부터는 Sentence와 문장을 구분하여 사용하겠습니다. BERT에서는 Sentence를 그저 **연속적으로 펼쳐진 텍스트**라는 의미로 사용합니다. 이는 문장의 일부분일 수도 있고 여러 개의 문장이 될 수도 있습니다.

아래 그림은 데이터가 모델에 입력되는 과정을 보여주는 이미지입니다.



<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514174-4d7bb900-be4b-11ea-923f-d0d0d474e1fc.png" alt="bert4"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

BERT는 레이블되지 않은 하나 혹은 두 개의 Sentence를 입력합니다. 이들은 BERT에서만 사용되는 스페셜 토큰과 함께 모델에 입력됩니다. 첫 번째 스페셜 토큰인 `[CLS]`는 입력 데이터의 시작을 알리는 역할을 합니다. `[CLS]`는 학습을 진행하면서 다음 Sentence를 예측하는 정보를 담은 은닉 벡터(Hidden vector)로 거듭나게 됩니다. 두 번째 스페셜 토큰은 `[SEP]`입니다. `[SEP]`는 입력 데이터가 두 개의 Sentence로 구성되는 경우 이 두 Sentence를 구분하는 역할을 합니다. 만약 Sentence가 하나로만 구성되었다면 입력 데이터의 맨 마지막에 위치하게 됩니다.

BERT에 입력되는 벡터는 세 가지 임베딩 벡터의 합으로 구성됩니다. 첫 번째는 3만 개의 단어로 이루어진 사전으로부터 가져온 단어 토큰 임베딩 벡터입니다. 두 번째는 `[SEP]`을 기준으로 앞에 있는 Sentence인지 뒤에 있는 Sentence인지를 구분하는 Segment 임베딩 벡터입니다. 마지막은 각 토큰이 어디에 위치하는 지를 표시하는 위치 임베딩 벡터입니다. 이 세 가지 벡터를 모두 더항 입력 벡터를 구성하게 됩니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514189-8156de80-be4b-11ea-8437-974e5b95fcb5.png" alt="bert5"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

## Pretraining

### MLM

BERT의 2가지 Pre-training 방법인 Masked LM과 NSP에 대해서 더 자세히 알아보겠습니다. 먼저 첫 번째 학습 방법인 **Masked LM(Masked Language Model, MLM)**입니다. 혹시 중-고등학교 암기 과목 시험을 준비하면서 *'빈칸 채우기 공부법'*을 해본 적이 있으신가요? 예를 들어 "기묘사화"에 대한 내용을 공부한다고 해보겠습니다. 텍스트는 위키피디아를 참조하였습니다.

> **기묘사화**는 1519년(중종 14) 음력 11월에 조선에서 남곤, 심정, 홍경주, 김전, 중종(中宗) 등이 "\_\_\_\_\_\_", 김식 등 신진사림의 핵심인물들을 몰아내어 죽이거나 혹은 귀양보낸 사건이다. "\_\_\_\_\_\_" 등의 세력 확장과 위훈 삭제에 대한 불만이 원인 중 하나였다.

빈칸에 공통적으로 들어갈 인물은 *"조광조"* 입니다. 이런 공부법이 절대적으로 좋은 공부법이라고는 할 수 없지만 단기적으로 정보를 암기하는 데는 좋았던 기억이 있습니다. 그래서 매번 다른 곳에 빈칸을 뚫어가면서 공부했었습니다. 다음 번에는 *"기묘사화"*에 빈칸을 뚫는다든지, *"위훈 삭제"*에 빈칸을 뚫는다든지 하는 것이지요.

BERT의 MLM도 위와 유사한 방식으로 언어를 학습합니다. [ELMo](https://yngie-c.github.io/nlp/2020/07/03/nlp_elmo/)까지의 언어 모델은 순차적으로 단어를 예측하는 방향으로 진행되었습니다. 순방향이든 역방향이든 일련의 시퀀스를 보고 그 다음 혹은 이전에 올 단어를 예측했었습니다. 하지만 MLM에서는 빈칸 채우기 방법을 통해 가운데 단어를 예측합니다. 실제로 약 15%의 단어를 임의로 마스킹한 뒤에 양방향으로(Bidirectional) 탐색하여 마스킹된 단어가 실제로 어떤 단어인지를 예측합니다. 

ELMo에서 biLM을 사용하여 양방향으로 예측한 단어 벡터를 이어붙이는(concatenate) 것은 단어 양쪽 문맥의 정보를 얻을 수 있었지만 양쪽 문맥을 동시에 볼 수는 없다는 한계점을 가지고 있었습니다. BERT는 마스킹을 한 뒤에 양방향 학습하는 것으로 이런 한계점을 해결할 수 있었습니다.

아래는 BERT가 *"Let's stick to improvisation in this skit"*문장을 MLM을 통해 단어를 학습하는 방법을 잘 나타내는 이미지입니다.

<p align="center"><img src="http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png" alt="bert6"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-bert/">jalammar.github.io</a></p>

위에서는 *"improvisation"*이 마스킹 단어로 선택되었습니다. 이렇게 선택된 단어는 `[MASK]`토큰이 되어 모델에 들어가게 되고 모델은 해당 단어에 들어갈 가장 적절한 단어를 확률로써 찾아냅니다. 이 단어가 *"improvisation"*이 되도록 모델이 학습을 진행하게 됩니다.

하지만 이러한 MLM Pre-trainining 방법을 그대로 사용하면 Fine-tuning에서 차이가 발생하게 됩니다. Fine-tuning에서는 입력 데이터에 마스킹을 하지 않기 때문입니다. 논문에서는 이런 차이를 줄이기 위해서 일종의 트릭을 사용하였습니다. 이 트릭에 의해서 Pre-training에서 마스킹하기로 정한 단어 중 80%만 마스킹 처리를 합니다. 나머지 20% 중 절반인 10%는 원래 단어로 두고 나머지 10%는 임의의 단어로 치환합니다. 논문에서는 *(마스킹 : 동일 단어 : 임의의 단어)*를 다른 비율에 대해서도 실험한 결과 80 : 10 : 10의 비율이 가장 잘 나왔음을 보여주고 있습니다.

> *"My dog is hairy"* $\rightarrow$ *"My dog is [MASK]"* - (80%)
>
> *"My dog is hairy"* $\rightarrow$ *"My dog is hairy"* - (10%)
>
> *"My dog is hairy"* $\rightarrow$ *"My dog is apple* - (10%)

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514778-38a12480-be4f-11ea-944b-074b3a1f5b79.png" alt="bert_mlm" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

### NSP

두 번째 학습 방법은 **NSP(Next Sentence Prediction)**입니다. NSP는 한 쌍의 문장이 들어왔을 때 뒤에 있는 문장이 바로 다음에 등장하는 문장인지를 이진 분류로 예측합니다. 위에서 사용했던 기묘사화를 예시로 다시 가져와 보겠습니다. 다음은 NSP가 True로 판단해야 하는 한 쌍의 문장입니다. (BERT에서 사용하는 sentence는 일반적인 의미의 '문장'과 다르지만 예시를 위해 2개의 문장을 사용하였습니다.)

> *S1 : 기묘사화는 1519년(중종 14) 음력 11월에 조선에서 남곤, 심정, 홍경주, 김전, 중종(中宗) 등이 조광조, 김식 등 신진사림의 핵심인물들을 몰아내어 죽이거나 혹은 귀양보낸 사건이다.*
>
> *S2 : 조광조 등의 세력 확장과 위훈 삭제에 대한 불만이 원인 중 하나였다.*

위 두 문장은 실제로 이어지는 문장이니 NSP가 True로 판단해야 합니다. 반대로 NSP가 False로 판단해야 하는 한 쌍의 문장은 다음과 같은 것입니다.

> *S1 : 기묘사화는 1519년(중종 14) 음력 11월에 조선에서 남곤, 심정, 홍경주, 김전, 중종(中宗) 등이 조광조, 김식 등 신진사림의 핵심인물들을 몰아내어 죽이거나 혹은 귀양보낸 사건이다.*
>
> *S2 : 전세계 최대 규모의 동영상 공유 사이트로서, 유튜브 이용자가 영상을 시청·업로드·공유할 수 있다.*

아래 문장은 유튜브의 위키피디아 설명을 참조한 것입니다. 실제로 두 문장은 이어져 쓰인 것이 아니기 때문에 NSP가 False로 판단해야 합니다. 

QA(Question & Answering)나 NLI(Natural Language Inference, 자연어 추론) 등 복잡한 태스크는 Sentence 사이의 관계를 이해하여야 풀 수 있습니다. 이런 이유 때문에 한 문장씩 입력하던 이전의 모델은 이런 태스크에서는 높은 성능을 보이지 못했습니다. 하지만 BERT는 복잡한 태스크까지 커버하기 위하여 NSP를 학습할 수 있는 장치를 마련하였습니다.

그 장치가 앞에서 보았던 `[CLS]`라는 스페셜 토큰입니다. 이 토큰은 모델에서 학습되면서 Sentence쌍이 실제로 다음에 올지 아닐 지를 판단합니다. 아래 이미지는 `[CLS]`토큰의 역할을 잘 보여주는 그림입니다.

<p align="center"><img src="http://jalammar.github.io/images/bert-next-sentence-prediction.png" alt="bert8"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-bert/">jalammar.github.io</a></p>

실제 Pre-training 과정에서는 입력할 Sentence 2개를 뽑을 때 50%는 실제로 이어지는 Sentence 쌍을, 나머지 50%는 말뭉치 내에 있는 랜덤한 Sentence를 가져오도록 합니다. 그리고 `[CLS]`의 은닉 벡터인 `[C]`를 통해서 바로 다음에 이어지는 문장인지(`IsNext`) 떨어져 있는 문장인지(`NotNext`) 를 예측하도록 합니다. NSP는 간단한 아이디어지만 이를 추가함으로써 QA나 NLI 등의 복잡한 태스크에서 기존 모델보다 훨씬 더 높은 성능을 보이도록 할 수 있었습니다. 아래는 NSP를 수행하는 과정을 이미지로 나타낸 것입니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514846-d0067780-be4f-11ea-9809-c3e43b8ad3f9.png" alt="bert_nsp1"  /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514847-d137a480-be4f-11ea-82be-d229bf75fbf8.png" alt="bert_nsp2"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics 강의자료</a></p>

## Fine-Tuning

이어서 BERT의 Fine-Tuning에 대해서 알아보겠습니다. Fine-tuning의 목적은 Pre-trained BERT를 목적에 맞는 태스크에 사용하기 위함입니다. Pre-trained BERT위에 하나의 층을 더 쌓아 학습시키면 태스크에 맞게 모델을 사용할 수 있습니다.

Fine-tuning 과정을 태스크에 따라 크게 4가지로 구분해 볼 수 있습니다. 아래 그림을 통해서 각 태스크에 대해 알아보겠습니다.

<p align="center"><img src="http://jalammar.github.io/images/bert-tasks.png" alt="bert_ft"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

(a)는 다양한 Sentence pair 분류와 관련된 태스크입니다. (b)는 감성분석과 같은 단문장 이진 분류와 관련된 태스크입니다. (c)는 주어진 문서와 질문이 주어지면 답에 대한 Sentence를 찾는 태스크입니다. (d)는 토큰마다 답을 내야 하는 개체명 인식(Named Entity Recognition, NER) 등의 태스크입니다. 

(a)와 (c)의 태스크는 입력 데이터를 2개의 Sentence로 구성합니다. 중간에 `[SEP]`이 들어가는 것을 보더라도 알 수 있습니다. (b)와 (d)의 태스크는 입력 데이터를 1개의 Sentence로 구성합니다.

아래는 각 태스크에 대하여 BERT와 다른 모델의 성능을 비교하여 나타낸 것입니다. BERT BASE 만으로도 이전에 발표된 모델에 비해 더 좋은 성능을 기록한 것을 볼 수 있습니다. BERT LARGE의 경우 CoLA, RTE등의 데이터셋에는 BASE 보다도 더욱 좋은 성능을 나타내며 SOTA를 달성하였습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514902-515e0a00-be50-11ea-9c00-14cab4375724.png" alt="bert_perf" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

## Ablation Study

아래는 Pre-training에서 NSP를 학습하는 경우와 그렇지 않은 경우의 성능을 비교한 것입니다. NSP를 수행하였을 때에는 그렇지 않을 때보다 QNLI, SQuAD의 태스크에서 성능 향상을 보이는 것을 확인할 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514948-908c5b00-be50-11ea-9a8c-921bf6bc72c8.png" alt="bert_nsp4" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

아래는 BERT 모델의 사이즈를 바꾸어가며 성능을 평가한 것입니다. $\#L$은 인코더 블록의 개수이고 $\#H, \#A$ 는 임베딩 벡터의 차원과 Attention Head의 개수입니다. 아래 그림에서 4번째에 있는 것이 BASE 모델이며 맨 아래에 있는 것이 LARGE 모델입니다. 아래 표에서 모델의 사이즈를 늘릴 수록 성능이 좋아지는 것을 볼 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514950-93874b80-be50-11ea-9d30-b06df2d67ab6.png" alt="bert_size1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

아래는 Fine-tuning과 Feature-based[^1]에 대해 각각의 성능을 비교한 것입니다. Fine-Tuning을 해준 경우가 가장 성능이 높은 것을 볼 수 있습니다. Feature-based를 사용할 때에는 마지막 4개의 은닉 벡터를 이어붙이(Concatenate)거나 선형결합하여 사용하는 것이 가장 높은 결과를 보이고 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/91357212-da2f6b80-e82b-11ea-94d6-80a034f6e6b2.png" alt="finetune_vs_feature" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

아래는 이 실험 결과를 잘 나타내주는 이미지입니다.

<p align="center"><img src="http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png" alt="bert_fb"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-bert/">jalammar.github.io</a></p>

## 의의

MLM, NSP와 같은 BERT만의 특이한 학습 방법은 이전의 모델을 모두 누르고 SOTA를 달성하는 데에 커다란 공을 세웠습니다. BERT를 시작으로 이를 더 좋게 만들거나 경량화 하거나 목적에 맞게 개량하는 과정에서 *"RoBERTa, AlBERT..."* 등 수많은 BERT모델이 탄생하게 됩니다. BERT는 이러한 파생 모델의 원조격으로서 자연어처리 분야의 전체적인 발전을 가져온 기념비적인 모델이라고 할 수 있습니다. 

[^1]: 추가로 학습하는 과정에서 업데이트 하는 대상이 단어 임베딩 벡터까지 포함될 경우 Fine-tuning이라고 하며 단어 임베딩은 고정한 채 신경망 파라미터만 학습하는 것을 Feature based 기법이라고 합니다. (출처 : 한국어 임베딩)