---
layout: post
title: 주성분분석(Principal Component Analysis)
category: Machine Learning
tag: Machine-Learning
---



해당 게시물은 [고려대학교 강필성 교수님의 강의](https://github.com/pilsung-kang/Business-Analytics-IME654-)를 바탕으로 작성한 것입니다.

# Principal Component Analysis

차원 축소의 목적은 주어진 태스크를 수행하는 모델을 만들기 위해서 원래의 데이터가 가진 정보를 최대한 보존하면서도 더욱 차원이 적은 데이터셋을 구성하는 것이었습니다. 이전 시간까지 알아본 전진 선택, 후진 제거, Stepwise, 유전 알고리즘 등은 특성 선택(Feature selection)을 위한 알고리즘이었습니다. 특성 선택은 원래 존재하는 특성 중에서 더 중요한 특성의 부분 집합만을 선택하는 방법입니다.

반대로 오늘 알아볼 주성분 분석, 다음 시간에 등장할 다차원 척도법 등은 특성 추출(Feature extraction)을 위한 알고리즘입니다. 특성 추출은 데이터가 가지고 있는 특성을 모두 보존하는 방향으로, 각 특성들을 결합하여 새로운 특성을 생성하는 방법입니다. 이 중 **주성분 분석(Principal Component Analysis, PCA)**은 원래 데이터를 사영시켰을 때 원래 데이터의 분산을 최대한 보존하는 기저 벡터를 찾는 방법입니다. 주성분분석을 이미지로 살펴보도록 하겠습니다.  



<p align="center"><img src="https://i.stack.imgur.com/lNHqt.gif" alt="pca"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">stackexchange.com</a></p>



위의 원래 데이터는 2차원 평면상에 펼쳐져 있습니다. 만약 이 데이터를 1차원인 직선상에 나타낸다면 어떤 직선을 선택하는 것이 가장 좋을 지를 알아내는 방법이 바로 주성분분석입니다. 위 그림에서는 분홍색 선과 이어지는 직선으로 결정되는 것을 볼 수 있습니다. 만약 다른 축을 선택했다면 어떻게 되었을까요? 아래 그림은 다양한 방향의 직선에 대해서 사영되는 데이터의 모습을 보여주고 있습니다.



<p align="center"><img src="https://i.stack.imgur.com/Q7HIP.gif" alt="pca2"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">stackexchange.com</a></p>



위 이미지에 등장하는 직선과 그에 사영되는 점을 보겠습니다. 위와 같이 분홍색과 이어지는 직선에서는 데이터가 퍼진 형태를 잘 보존하고 있는 것을 알 수 있습니다. 반대로 그와 직교하는 직선의 방향이 에서는 모든 점이 가운데로 모여 데이터가 퍼져있는 모습을 잘 나타내지 못하는 것을 볼 수 있습니다. 이러한 축을 주성분 이라고 하며 주성분에 데이터를 사영하는 것이 주성분 분석의 목적입니다.



## Background Knowledge

주성분분석에 사용되는 수리적 배경에 대해서 알아보겠습니다. 첫 번째는 **공분산 행렬(Covariance matrix)** $\text{Cov}$ 입니다. $\text{Cov}$ 는 $n \times n$ 사이즈의 행렬입니다. 수식으로는 다음과 같이 정리됩니다.



$$
\text{Cov}(\mathbf{X}) = \frac{1}{n}(\mathbf{X} - \bar{\mathbf{X}})(\mathbf{X} - \bar{\mathbf{X}})^T
$$



위 식에서 $\mathbf{X}_{d \times n}$ 는 데이터셋을 Column-wise vector의 행렬로 나타낸 것입니다. 일반적으로 데이터셋을 행렬로 나타낼 때에는 인스턴스(Instance)를 각 행(Row)에,  특성을 각 열(Column)에 배치하는 Row-wise vector 방식으로 표현합니다. $d$ 는 특성의 개수이며 $n$ 은 데이터의 개수입니다. 하지만 공분산 행렬을 구할 때에는 이를 전치하여 나타낸 Column-wise vector 방식의 표현을 사용합니다. 이렇게 구해진 공분산 행렬의 사이즈는 $n \times n$ 이며 대칭 행렬(Symmetric matrix)입니다. 이렇게 구해진 공분산 행렬의 대각 성분(Diagonal term)을 모두 더한 Trace를 구하면 원래 데이터셋의 분산을 구할 수 있습니다.

다음으로 알아야 하는 개념은 [사영(Projection)](https://yngie-c.github.io/linear%20algebra/2020/09/12/projection/)입니다. 아래 그림은 특정 벡터 $\vec{a}$를 다른 벡터 $\vec{b}$ 에 사영했을 때의 벡터를 $\vec{a_1}$ 으로 나타낸 그림입니다.

<p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/9/98/Projection_and_rejection.png" alt="projection" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://en.wikipedia.org/wiki/Vector_projection">wikipedia.org/wiki/Vector_projection</a></p>



이 때 사영의 과정을 미지수 $p$로 나타내면 $\vec{a_1} = p\vec{b}$ 라고 나타낼 수 있고 $\vec{a_2} = \vec{a} - p\vec{b}$로 나타낼 수 있으므로 다음과 같은 식이 성립하게 됩니다.


$$
\begin{aligned}
\vec{a_2}^T\vec{b} = (\vec{a} - p\vec{b})^T\vec{b} &= 0 \\
\vec{a}^T\vec{b} - p\vec{b}^T\vec{b} &= 0 \\
\end{aligned}\\
\begin{aligned}
&\therefore p = \frac{\vec{a}^T\vec{b}}{\vec{b}^T\vec{b}} \\
&\because \vec{a_2} \perp \vec{b}
\end{aligned}
$$


구해낸 $p$를 사용하여 $\vec{a_1}$을 표현할 수 있습니다.


$$
\vec{a_1} = p\vec{b} = \frac{\vec{a}^T\vec{b}}{\vec{b}^T\vec{b}}\vec{b}
$$


만약 $\vec{b}$ 가 단위 벡터(unit vector)라면 $\Vert\vec{b}\Vert = 1$ 이므로 다음과 같은 식이 성립하게 됩니다.


$$
\vec{a_1} = p\vec{b} = (\vec{a}^T\vec{b})\vec{b} \\
\because \vec{b}^T\vec{b} = \Vert\vec{b}\Vert^2 = 1
$$


마지막으로 알아야 할 개념은 [고윳값(eigenvalue)과 고유벡터(eigenvector)](https://yngie-c.github.io/linear%20algebra/2020/09/30/eigenvectors/)입니다. 행렬 $A$를 사용하여 모든 벡터를 선형 변환(Linear transformation)하면 방향이 바뀌는 벡터가 있고 그렇지 않은 벡터가 있습니다. 아래 그림을 보겠습니다.

<p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif" alt="linear_trans" style="zoom: 80%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://commons.wikimedia.org/wiki/File:Eigenvectors-extended.gif">commons.wikimedia.org</a></p>

위 그림에서 빨간색으로 나타나는 벡터는 선형 변환에 의해서 방향이 바뀝니다. 반대로 파란색으로 나타나는 벡터는 크기는 변할지언정 방향은 변하지 않고, 핑크색으로 나타나는 벡터는 크기도 방향도 변하지 않는 것을 볼 수 있습니다. 이들처럼 방향이 변하지 않는 벡터를 **고유벡터(Eigenvector)**라고 하며 파란색 벡터처럼 고유벡터의 크기가 원래의 벡터보다 $k$ 배 변할 때 이 값 $k$를 해당 고유벡터의 **고윳값(Eigenvalue)**라고 합니다.



## How to find PC

위 지식을 사용하여 주성분을 어떻게 찾는 지에 대해서 알아보겠습니다. 첫 번째 과정으로 데이터셋의 평균이 0이 되도록 변환(Centering)합니다. 위에서 공분산 행렬을 구했던 과정과도 비슷합니다. $\mathbf{X} - \bar{\mathbf{X}}$ 를 해주어 모든 데이터셋의 평균을 0으로 만들어줍니다. 지금부터는 이렇게 변환된 행렬을 다시 $\mathbf{X}$라고 나타내겠습니다.

우리의 목표는 차원을 축소한 행렬의 분산을 구하는 것입니다. 따라서 변환한 행렬 $\mathbf{X}$를 특정 기저 벡터 $\mathbf{w}$ 에 사영한 행렬의 공분산 행렬을 구합니다. 식은 위에서 구했던 것과 동일하므로 아래와 같이 나타낼 수 있습니다. 아래 식에서 $\mathbf{S}$는 변환된 행렬 $\mathbf{X}$ 의 표본 공분산 행렬(Sample covariance matrix)입니다.


$$
V = \frac{1}{n}(\mathbf{w}^T\mathbf{X})(\mathbf{w}^T\mathbf{X})^T = \frac{1}{n}\mathbf{w}^T\mathbf{X}\mathbf{X}^T\mathbf{w} = \mathbf{w}^T\mathbf{S}\mathbf{w}
$$


우리의 목표는 $V$를 최대화 하는 지점, 즉 $\max \mathbf{w}^T\mathbf{S}\mathbf{w}$ 를 찾는 것이므로 **라그랑주 승수법(Lagrangian multiplier)**을 사용하여 아래와 같이 구할 수 있습니다.


$$
\max \mathbf{w}^T\mathbf{S}\mathbf{w} \qquad s.t. \mathbf{w}^T\mathbf{w} = 1 \\
L = \mathbf{w}^T\mathbf{S}\mathbf{w} - \lambda(\mathbf{w}^T\mathbf{w} - 1) \\
\frac{\part L}{\part \mathbf{w}} = 0 \Rightarrow \mathbf{S}\mathbf{w} - \lambda\mathbf{w} = (\mathbf{S} - \lambda\mathbf{I})\mathbf{w} = 0
$$


예를 들어, 2차원 데이터에 대하여 $\mathbf{S}, \lambda$ 가 각각 아래와 같이 나왔다고 해보겠습니다.


$$
\mathbf{S} = \left[\begin{array}{cc} 0.6779 & -0.7352 \\ 0.7352 & 0.6779 \end{array} \right] \qquad \lambda = \left[\begin{array}{cc} 1.2840 & 0.0491 \end{array} \right]
$$


이중 $\lambda_1 = 1.2840, \lambda_2 = 0.0491$ 이라 하면 $e_1 = \left[\begin{array}{cc} 0.6779 & 0.7352 \end{array} \right]^T$ 를 주성분으로 선택했을 때 보존되는 분산의 비율을 아래와 같이 구할 수 있습니다.


$$
\frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{1.2840}{1.2840 + 0.0491} \approx 0.96
$$


위와 같이 2차원 데이터를 1차원으로 축소하더라도 원래 데이터 분산의 96% 정도를 보존하는 것을 알 수 있습니다.

## Issues

다음으로 주성분 분석을 수행할 때 고려해야 할 몇 가지 이슈에 대해서 알아보겠습니다. 몇 개의 차원으로 축소할 것인가, 즉 몇 개의 변수를 택할 것인가에 대한 문제입니다. 사실 이 문제에 대한 명시적인 해는 없습니다. 해를 찾기 위한 방법으로는 정성적인 방법과 정량적인 방법이 있습니다. 먼저 해당 도메인 지식이 풍부한 전문가가 판단하는 정성적 방법이 있습니다.

정량적인 방법은 2가지로 나뉩니다. 첫 번째는 엘보우 지점(Elbow point)을 찾는 것입니다. 이 지점은 아래의 $n$개의 주성분을 선택할 때 $n$번째의 주성분이 보존하는 분산의 비율을 나타낸 것입니다. 아래 그림에서 첫 번째 주성분은 원래 데이터 분산의 약 10%를 보존하고 있으며, 점점 주성분이 추가될 때마다 그 주성분에 의해 추가되는 분산의 비율은 줄어드는 것을 볼 수 있습니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/94834445-5af50f00-044b-11eb-8a79-3f1b2d2944b7.png" alt="elbow_point"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">github.com/pilsung-kang/Business-Analytics-IME654</a></p>

위 그림에서 빨간색 표시가 되어있는 지점, 즉 하나의 주성분을 더 추가하더라도 분산을 많이 보존하지 못하는 지점을 엘보우 지점이라고 하며 엘보우 지점의 바로 앞 단계 만큼의 주성분을 보존하는 것이 하나의 방법입니다. 두 번째 방법은 보존 하고자 하는 분산 비율의 기준점을 정하고 그 기준을 넘기는 지점까지의 주성분을 선택하는 방법입니다. 위에서 보았던 그림을 누적 그래프로 나타낸 그림입니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/94836458-f8e9d900-044d-11eb-8441-5a81f8f75af3.png" alt="cumulative"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">github.com/pilsung-kang/Business-Analytics-IME654</a></p>

위 그림에서는 원래 데이터 분산의 80%를 기준점으로 사용하였고 그 이상의 분산을 보존하는 지점까지만 주성분을 선택합니다. 

다음 이슈는 주성분 분석에서 고려해야 할 점입니다. 주성분 분석은 대상이 되는 데이터가 가우시안 분포(Gaussian distribution)를 이루고 있음을 가정하고 있습니다. 그렇기 때문에 가우시안 분포를 가지지 않는 데이터에 주성분 분석을 사용한다면 결과물이 유효하다고 할 수 없습니다.

마지막 이슈는 주성분 분석이 데이터의 결정 경계를 찾기 위해 사용하는 방법은 아니라는 점입니다. 아래 그림은 특정 데이터셋에 대하여 주성분 분석을 사용했을 때 찾아지는 기저와 분류를 위한 방법인 FLDA(Fisher's Linear Discriminant Analysis)를 사용했을 때 찾아지는 축을 나타낸 것입니다.



<p align="center"><img src="https://user-images.githubusercontent.com/45377884/94838543-b675cb80-0450-11eb-9fa8-f513787144c4.PNG" alt="pca_vs_lda"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">github.com/pilsung-kang/Business-Analytics-IME654</a></p>

위 그림의 데이터를 더 잘 분류할 수 있는 방법은 FLDA입니다. PCA는 분산을 최대화 하는 기저를 찾는 것 뿐이므로 결정 경계를 찾는 데에 적절한 방법론은 아님을 알 수 있습니다.

