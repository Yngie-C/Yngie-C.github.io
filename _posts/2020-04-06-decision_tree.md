---
layout: post
title: 결정 트리 (Decision Tree)
category: Machine Learning
tag: Machine-Learning
---

 본 포스트는 [카이스트 문일철 교수님의 강의](https://www.edwith.org/machinelearning1_17/joinLectures/9738) 를 바탕으로 작성하였습니다. 책은 [핸즈온 머신러닝](http://www.yes24.com/Product/Goods/59878826) 을 참조하여 작성하였습니다. 다양한 결정 트리 알고리즘에 대한 부분은 [해당 티스토리 블로그](https://ai-times.tistory.com) 를 참조하였습니다.



# Decision Tree

우리가 살고 있는 세계는 완벽한 세계에 살고있지 않다. 소음도 많고 우리가 관측할 수 없는 정보도 수없이 많다. 때문에 Rule Based Learning은 좋은 방법이 되지 못하고 더 좋은 학습 방법을 찾아내야 한다. 그 중에 가장 쉬운 방법 중 하나가 **결정트리(Decision Tree)** 이다. 

이번 게시물에서는 UCI 신용평가 Dataset을 통해서 결정트리에 대해서 알아보도록 한다. 이 데이터셋에는 총 690개의 인스턴스가 15개의 특성을 가지고 있다. 그 중 각각 첫 번째 특성(A1) 과 아홉번째 특성(A9)으로 나누어지는 경우를 비교해보자. 먼저 a와 b의 경우가 있는 특성 A1으로 나눌 때 a에는 98건의 +(positive label)가  112건의 -(negative label)가 속하게 된다. b에는 206+, 262-가 속하며 a도 b도 아닌 12건(3+.9-)의 데이터가 있다. t와 f의 경우가 있는 A9 특성으로 나눌 때에는 t에 284+,77- 이 f에 23+,306- 이 속한다.



## Entropy & Information Gain

나뉘는 결과가 위와 같다면 어떤 특성을 기준으로 나누는 것이 좋을까? 한번 나눌 때 데이터셋의 **불확실성(Uncertainty)** 을 많이 줄여줄수록 좋은 특성이라 할 수 있다. 이 때 데이터셋의 불확실성을 측정할 수 있도록 만든 것이 **엔트로피(Entropy)** 이다. 엔트로피에 대해서 알아보기 전에 먼저 정보량에 대해 알아보도록 하자. 정보량 $(I)$ 를 표현하는 식은 다음과 같다.

<br>
$$
I(X) = \log_2 \frac{1}{p(X)}
$$


위 식에서 $p(X)$ 는 사건 $X$ 가 발생할 확률이다. 따라서 특정한 사건이 가진 정보량은 사건이 일어날 확률에 반비례하는 경향을 보인다. 엔트로피는 이 정보량의 **평균** 을 나타내는 수치이다. 엔트로피 $H(X)$ 는 다음과 같은 식을 사용하여 구할 수 있다.

<br>
$$
H(X) = -\sum_X P(X=x) \log_b P(X=x)
$$
이렇게 해서 구해진 엔트로피가 높을수록 데이터셋 더 불확실하다는 것을 의미한다. 이 엔트로피 식에 조건을 배당하면 조건 엔트로피에 대한 식도 도출해낼 수 있다. 아래 조건 엔트로피 식을 통해 주어진 특성에 대한 엔트로피를 구해낼 수 있다.

<br>
$$
H(Y\vert X) = \sum_X P(X=x) \log_b H(Y\vert X=x) \\
= \sum_X P(X=x) \{ -\sum_Y P(Y=y\vert X=x) \log_b H(Y=y\vert X=x)\}
$$
위 식을 이용하여 원 데이터, A1로 나누었을 때, A9로 나누었을 때의 엔트로피를 계산해보자. 그리고 $A_i$ 특성으로 나누기 전의 엔트로피에서 나누고 난 뒤의 엔트로피를 빼준 것을 **Information Gain(정보 획득도, IG)** 라고 한다. IG가 가장 높은 특성을 선택하여 두 개의 클래스로 분할하는 것이 좋다.

<br>
$$
H(Y) = -\sum_{Y \in \{+,-\}} P(Y=y) \log_2 P(Y=y) \\
H(Y\vert A1) = \sum_{X \in \{a,b,?\}} \sum_{Y \in \{+,-\}} P(A1= x, Y=y) \log_2 \frac {P(A1=x)}{P(A1= x, Y=y)} \\
H(Y\vert A9) = \sum_{X \in \{t,f\}} \sum_{Y \in \{+,-\}} P(A9= x, Y=y) \log_2 \frac {P(A9=x)}{P(A9= x, Y=y)} \\
IG(Y,A_i) = H(Y) - H(Y \vert A_i)
$$



## Variable Decision Tree & Problem

결정트리의 종류에는 ID3, C4.5, CART 등 다양한 것들이 있다.

**ID3(Iterative Dichotomiser 3)** : 먼저 오픈 노드를 하나 만들고 그 안에 모든 인스턴스를 다 넣는다. 쪼갤 오픈 노드를 선택하고 인스턴스를 나눌 기준이 되는 최적의 특성을 선택한다 특성의 클래스에 맞게 인스턴스를 분리한다. 만약 한 노드에 한쪽 레이블만을 가진 데이터셋이 있을 경우 나누는 것을 종료한다.

**C 4.5** : ID3가 가진 몇 가지 단점을 보완한 알고리즘이다. 먼저 ID3에서는 범주형 속성만을 다룰 수 있었는데 비해 C 4.5에서는 수치형 속성을 다룰 수 있다. 두 번째로 결정 트리의 깊이 문제를 해결할 수 있다. ID3에서는 순수한 노드가 나올 때까지 가지를 나누기 때문에 과적합(Overfitting) 문제가 쉽게 발생한다. C 4.5 알고리즘에서는 가지치기(Pruning) 과정을 통해 트리가 너무 깊게 형성되지 않도록 한다. 다음으로 C 4.5는 결측치를 처리하고 속성에 서로 다른 가중치를 부여하며 계산을 효율적으로 진행하는 장점도 가지고 있다.

**CART(Classification And Regression Tree)** : CART 알고리즘이 진행되는 방향은 ID3와 동일하다. ID3에서 속성을 선택하기 위한 기준으로 엔트로피의 변화(정보 획득도, IG)를 사용한 것에 비해 CART에서는 엔트로피 매트릭스를 사용한다는 차이점을 가지고 있다. CART에서 등장하는 **지니 불순도(Gini Impurity)** 라는 새로운 수치에 대해 알아보자. 지니 불순도 $(G_i)$ 를 구하는 식은 아래와 같다.

<br/>
$$
G_i = 1 - \sum^n_{k=1} {p_{i,k}}^2
$$
먼저 훈련 세트를 하나의 특성 $k$ 의 임곗값인 $t_k$ 를 사용하여 두 개의 서브셋으로 나눈다. 여기서 $k$ 와 $t_k$ 의 순서쌍 $(k, t_k)$ 는 아래의 비용 함수(Cost function)를 최소화하는 지점으로 결정된다.

<br/>
$$
J(k, t_k) = \frac{m_{\text{left}}}{m}G_{\text{left}} + \frac{m_{\text{right}}}{m}G_{\text{right}} \\
G_{\text{left/right}} : \text{왼쪽 / 오른쪽 서브셋의 불순도} \quad 
m_{\text{left/right}} : \text{왼쪽 / 오른쪽 서브셋의 샘플 수}
$$


**CHAID(Chi-squared Automatic Interaction Detection)** : 이산형 목표변수에 대한 카이제곱 $(\chi^2)$ -검정과 연속형 목표변수에 대한 F-검정을 이용하여 다지 분리(Multiway split)을 수행하는 알고리즘이다. 각 검정에 대한 수치를 기준으로 노드를 분할한다. 이 수치가 임곗값보다 낮을 경우 하위 노드를 병합하는 과정을 통해 CART와 달리 과적합을 면할 수 있다. 



