---
layout: post
title: Seq2Seq (Sequence to Sequence) 와 Attention
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [Jay alammar의 깃허브](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) , 그리고  [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# Seq2Seq(Sequence to Sequence)

**Seq2Seq(Sequence to Sequence)**란 아이템의 시퀀스를 입력받아 또 다른 시퀀스를 내놓는 모델을 통칭하여 일컫는 말입니다. 여기서 아이템이란 문장이라는 시퀀스를 구성하는 단어가 될 수도 있고, 동영상이라는 시퀀스를 구성하는 이미지가 될 수도 있습니다. 항상 입력하는 아이템의 개수와 출력하는 아이템의 개수가 같을 필요는 없습니다. 아래 그림에서도 3개의 아이템으로 구성된 시퀀스가 입력되었지만 4개의 아이템으로 구성된 시퀀스가 출력되는 것을 볼 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86036190-7a5d5400-ba78-11ea-8f18-c202b970eeae.gif" alt="seq2seq_1"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

자연어처리에서 Seq2Seq의 대표적인 태스크(Task)로는 **기계 번역(Machine translation)**이 있습니다. 아래는 위 이미지를 기계번역 관점에서 구체화한 이미지입니다. 3개의 단어로 이루어진 프랑스어 문장을 4개의 단어로 이루어진 영어 문장으로 번역하고 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86037788-e2149e80-ba7a-11ea-9ac1-77a6646c60e2.gif" alt="seq2seq_2"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

## Encoder - Decoder

Seq2Seq 모델은 크게 인코더(Encoder)와 디코더(Decoder)의 구조로 이루어져 있습니다. 이 중 인코더는 입력한 시퀀스의 정보를 어떻게 압축하여 처리할 지를 담당합니다. 디코더는 인코더가 압축하여 넘겨준 정보를 어떤 식으로 반환해 낼 지에 대한 역할을 합니다.

조금 더 자세히 설명하면 인코더는 입력 시퀀스에 있는 각각의 아이템을 컴파일(Compile)하여 하나의 벡터, 즉 문맥(Context) 벡터로 표현합니다. 시퀀스를 모두 읽어낸 후에는 생성한 문맥 벡터를 디코더에게 넘겨주게 되지요. 디코더는 이를 받아 새로운 아이템 시퀀스를 출력하게 됩니다. 아래 동영상은 인코더와 디코더가 어떤 역할을 수행하는지를 잘 보여주고 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86038256-99a9b080-ba7b-11ea-8207-9db61a0d42bf.gif" alt="seq2seq_3"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>



기계 번역 태스크라면 아래와 같이 적용될 것입니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86039320-3a4ca000-ba7d-11ea-8cd7-df00e7d120a4.gif" alt="seq2seq_4"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

## RNN(Recurrent Neural Network)

Seq2Seq 모델의 가장 고전적인 사례는 **RNN(Recurrent Neural Network, 순환 신경망)**입니다. RNN의 은닉층(Hidden layer)에서는  은닉층 위치에 해당하는 아이템에 대해 입력 벡터와 이전 은닉층이 출력한 벡터(Hidden state vector)를 동시에 입력받습니다. 이를 계산하여 나온 출력값은 출력이 될 뿐만 아니라 갱신된 은닉 상태 벡터(Hidden state vector)로써 다음 인풋과 함께 은닉층에 입력됩니다. 아래는 RNN이 첫 번째 Time step, 즉 시퀀스 내에 있는 첫 번째 아이템을 입력받을 때 동작하는 방식을 이미지로 나타낸 것입니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86039628-a929f900-ba7d-11ea-8a4e-09e945f10d12.gif" alt="RNN_1"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

첫 번째 Time step에서는 0번째 은닉 상태 벡터와 첫 번째 아이템의 벡터가 입력됩니다. 두 벡터를 입력받은 은닉층은 이를 계산하여 은닉 상태 벡터를 새롭게 갱신합니다(태스크마다 출력 벡터를 내놓는 시점은 달라집니다). 위에서 사용했던 예시를 통해서 각 Time step마다 은닉 상태 벡터가 갱신되는 모습을 볼 수 있습니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86040604-4e919c80-ba7f-11ea-8bd0-8fcba7e224fd.gif" alt="seq2seq_5"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

위 이미지를 조금더 자세히 나타낸 것이 아래 이미지 입니다. 인코더에서는 각 아이템이 입력될 때마다 은닉 상태 벡터가 업데이트 되고 있는 것을 볼 수 있습니다. 인코더는 가장 마지막 아이템이 입력되고 난 후에 생성된 은닉 상태 벡터를 디코더에게 넘겨주게 됩니다. 디코더는 이 벡터를 받아 일련의 연산을 거쳐 적절한 아이템을 반환합니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif" alt="seq2seq_6"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

## Attention

RNN의 가장 큰 단점은 장기 의존성(Long-term dependency) 문제입니다. 순차적으로 아이템이 입력되는 RNN의 구조 특성상 시퀀스 뒤쪽에 있는 아이템의 영향을 더 많이 받을 수 밖에 없습니다. 특히 시퀀스가 길어지면 시퀀스 앞쪽에 있는 단어의 영향이 거의 사라지는데 이것이 바로 장기 의존성 문제입니다. 

장기 의존성 문제를 해결하기 위해 Vanilla RNN의 구조를 변형한 LSTM(Long-term Short Memory, 장단기 기억망)이나 GRU(Gated Recurrent Unit)등이 제시되었습니다. 이런 변형 모델이 문제를 줄여주기는 했지만 순차적으로 입력되는 완벽하게 해결하지는 못했습니다.

**Attention**은 이를 해결하기 위해서 고안된 개념입니다. RNN구조가 보여주고 있는 문제는 문맥(Context)의 정보를 가지고 있는 은닉 상태 벡터(Hidden state vector)가 하나였기 때문에 병목현상이 발생한다는 것이었습니다. 하지만 Attention은 모델이 아이템을 출력할 때마다 입력 시퀀스에서 어떤 아이템을 주목(Attention)해야하는 지를 가중치 벡터로 표시해 줍니다.

<p align="center"><img src="https://lh3.googleusercontent.com/ZDgwFfcgXOkHB-e4tYz1-OvwfP3eXjGJ3l3LqFnxuulbfN1ufFNNZR2NVoWYlnFIuhAptt2WEiw9SPYoSyZ_799RiONKJOro2gSUH6I=w1440-rw-v1" alt="attention_connect_image"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory">DeepMind.com</a></p>

Attention에는 두 가지 모델이 있습니다. 모델을 발표한 사람의 이름을 따서 Bahadanau Attention과 Luong Attention 이라고 부릅니다. Bahadanau의 모델은 Attention 가중치(Score)를 학습하는 신경망이 따로 있으며 Luong의 모델은 유사도를 측정하여 Attention 가중치를 만들어냅니다. 두 모델 간의 성능 차이가 거의 없기 때문에 일반적으로는 Luong attention을 사용합니다.

아래는 Attention이 위에서 보았던 RNN과 어떤 점에서 다르게 작동하는 지를 보여주는 이미지입니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif" alt="seq2seq_7"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

위 그림으로부터 인코더가 더 이상 마지막 은닉 상태 벡터만을 디코더에 입력하지 않습니다. 대신 각 Time step 마다 생성되는 은닉 상태 벡터를 보관하고 있다가 문장이 끝나면 생성된 모든 은닉 상태 벡터를 디코더에 넘겨줍니다. 아이템이 입력될 때마다 생성되는 은닉 상태 벡터를 모두 넘겨주기 때문에 디코더에 더 많은 정보를 넘겨줄 수 있게 됩니다. 그리고 앞쪽 아이템에 의해서 생성된 은닉 상태 벡터는 앞쪽 아이템의 정보를 잘 간직하고 있기 때문에 장기 의존성 문제를 해결할 수 있습니다.

그렇다면 디코더는 각각의 은닉 상태 벡터를 어떻게 활용하는 것일까요? 아래는 4번째 Time step, 즉 입력 시퀀스 아이템에 의해서 생긴 은닉 상태 벡터를 디코더가 받은 후에 일어나는 일을 순차적으로 나타낸 것입니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86044868-ae8b4180-ba85-11ea-9fee-2977edfd47ce.gif" alt="attention_1"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

디코더는 먼저 각각의 은닉 상태 벡터를 살펴봅니다. 이 벡터들은 이전 Time step의 은닉 상태 벡터와 입력 시퀀스의 아이템에 의해서 생성되었기 때문에 그 벡터를 생성할 시점에 입력된 아이템의 정보를 가장 많이 가지고 있습니다. 디코더의 은닉 상태 벡터는 각각의 은닉 상태 벡터와 내적한 값(Attention score)에 소프트맥스를 취해줍니다.

소프트맥스의 함숫값, 즉 위 그림에서 $(0.96, 0.02, 0.02)$ 는 각 은닉 상태 벡터에 가중치가 됩니다. 이 값은 각각의 은닉 상태 벡터와 곱해지고 최종적으로 이를 모두 더한 새로운 컨텍스트 벡터(Sum-up weighted vector)를 생성하게 됩니다. 즉 2번 단계에서 계산되는 내적값이 클수록 디코딩되는 아이템 입장에서 중요한 은닉 상태 벡터이고, 작을수록 중요도가 떨어지는 벡터로 판단하게 되는 것입니다.

이렇게 생성된 컨텍스트 벡터는 디코더의 은닉 상태 벡터와 Concatenation(옆으로 붙임)하여 사용합니다. 그래서 출력 아이템을 생성하기 위해 입력되는 벡터의 사이즈는 두 벡터의 사이즈를 더한 것이 됩니다. 그리고 이를 각 출력 아이템의 FFNN(Feed-forward neural network)에 넣어 출력 아이템의 임베딩 벡터를 도출하게 됩니다. 이 과정을 도식화하면 아래와 같이 나타낼 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86046130-a59b6f80-ba87-11ea-8fe4-358b7a3b6a7f.gif" alt="attention_2"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

위 그림에서 `<END>`는 디코더가 작동하는 시점을 알리는 신호의 역할을 합니다. $h_\text{init}$는 각 은닉 상태 벡터를 내적하여 새로운 문맥 상태 벡터 $C_4$를 생성합니다. 디코더의 첫 번째 Time step의 은닉층이 생성한 $h_4$와 이 $C_4$를 Concatenation하게 되지요. 이렇게 생성된 벡터 $[C_4;h_4]$ 는 FFNN을 거쳐 가장 적합한 아이템 *"I"*의 임베딩 벡터를 출력하게 됩니다.

다음 Time step에서는 이전 단계의 은닉층이 출력하는 은닉 상태 벡터, 즉 $h_4$가 이전 단계에서 $h_\text{init}$과 같은 역할을 하게 됩니다. 그리고 이전 단계의 `<END>`와 같은 역할은 이전 단계의 출력 벡터인 *"I"*의 임베딩 벡터가 맡게 됩니다. 동일한 과정을 거치면 *"am"*의 임베딩 벡터가 출력됩니다. 이런 과정을 계속 반복하면 *"a", "student"*의 임베딩 벡터가 생성되며 디코더는 `<END>`혹은 `<END>`벡터를 생성할 때까지 이 과정을 반복한 후에 문장 생성을 종료합니다.

위 과정에서 출력 시퀀스의 각 아이템이 생성될 때 인코더의 은닉 상태 벡터에게 받은 Attention(주목도)를 시각화하면 다음과 같이 나타낼 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86047018-29a22700-ba89-11ea-98ee-a90b2fb70a23.gif" alt="seq2seq_9"  /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">jalammar.github.io</a></p>

## 의의

RNN구조든 Attention이든 자연어처리에서 Seq2Seq 구조의 발전은 기계 번역 태스크의 커다란 발전을 가져왔습니다. 게다가 RNN의 구조적 한계점을 개선한 Attention이 등장하면서 긴 시퀀스에 대해서도 높은 성능을 달성하였습니다. 게다가 Attention은 RNN의 구조상 효율적으로 할 수 없었던 GPU의 병렬 처리를 개선하여 계산 시간도 많이 단축하는 결과를 가져왔습니다.

기술 발전 과정의 관점에서 보더라도 이후 Self-Attention을 활용한 [트랜스포머(Transformer)](https://yngie-c.github.io/nlp/2020/07/01/nlp_transformer/)가 등장하게 되는 배경이 되는 아이디어로서의 중요한 역할을 담당하고 있습니다. 