---
layout: post
title: RNN
category: Deep Learning
tag: Deep-Learning
---



본 포스트의 내용은 [cs231n 강의자료](http://cs231n.stanford.edu/syllabus.html) 와 책 [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) 를 참고하였습니다.



# RNN (Recurrent Neural Network)

우리에게 주어지는 몇 가지 데이터는 시퀀스(Sequence)의 형태로 구성되어 있습니다. 예를 들면, *"I am a student"* 라는 

일반적인 뉴럴 네트워크는 아래 그림의 맨 왼쪽에 위치한 도식처럼 하나의 입력층(Input layer)과 하나의 은닉층(Hidden-layer)과 하나의 출력층(Output-layer)으로 이루어져 있다.  아래 그림에서 맨 왼쪽을 제외한 그림은 입력 혹은 출력 데이터가 시퀀스로 이루어진 경우를 나타낸다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85701189-18b78580-b718-11ea-93c5-b63aa5b64186.png" alt="rnn1" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

두 번째처럼 하나의 데이터가 입력되고 시퀀스 데이터가 출력되는 Task에는 Image Captioning이 있다. 입력 데이터는 그림 한 장이지만 그로부터 한 단어 이상으로 된 문장(Sequence)이 출력된다. 가운데처럼 시퀀스 데이터를 입력하여 하나의 데이터를 출력하는 Task에는 감성 분석(Sentiment Analysis)가 있다. 한 문장 혹은 여러 문장으로 이루어진 자연어 시퀀스로부터 이 문장이 긍정인지, 부정인지 (혹은 중립인지)를 판단하게 된다. 네 번째와 같은 형태로 입, 출력이 모두 시퀀스인 Task는 기계 번역(Machine Translation)이 있다. 특정 언어의 시퀀스를 입력받아 다른 언어의 시퀀스를 출력한다. 마지막과 같은 형태의 신경망에는 프레임 레벨에서 진행되는 영상 분류(Video classification on frame level)가 있다.

이런 시퀀스 데이터를 다루기 위해서 사용되는 것이 RNN(Recurrent Neural Network)이다. Recurrent는 '주기적으로 일어난다', '순환한다'는 뜻으로 RNN을 '순환 신경망'으로 번역하여 쓰기도 한다.

## RNN의 구조

NN에 왜 R을 붙였는지에 대한 이유는 아래 그림에 나타나 있는 RNN의 기본 구조를 통해서 알 수 있다.

 <p align="center"><img src="https://user-images.githubusercontent.com/45377884/85712025-d9dafd00-b722-11ea-96a4-4c393d34c24f.png" alt="rnn2" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

위 그림에서 나타난 RNN의 기본 구조는 일반적인 신경망 구조와는 다르게 은닉층에서 뻗어나가 다시 은닉층으로 돌아오는 하나의 흐름을 더 가지고 있다. 이 순환 구조를 펼치면 다음과 같은 그림으로 나타나게 된다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85719759-b0be6a80-b72a-11ea-87f5-32f9dad18656.png" alt="rnn3" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

RNN과 이전에 살펴본 신경망의 가장 큰 차이점은 계층이 없다는 것이다. RNN의 각 계층은 해당 계층으로의 입력 $x_t$와 이전 RNN계층이 내놓는 출력을 동시에 입력받는다. 이를 바탕으로 해당 층에서의 출력값을 계산하게 되며 출력값을 내보냄과 동시에 다음 RNN계층에 전달하게 된다. 이것을 수식으로 나타내면 다음과 같다.


$$
h_t = f_W (h_{t-1}, x_t)\\
h_t= \tanh (h_{t-1}W_h + x_tW_x+b)
$$


위 식에서 $x, h$ 는 각각 입, 출력 벡터이며 $W$ 는 이 벡터에 곱해지는 가중치이다. $b$ 는 편향을 나타내는 벡터이며 이 값들에 활성화 함수인 $\tanh$ 를 취해주어 새로운 은닉 상태 벡터인 $h$를 구하게 된다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85719977-ee22f800-b72a-11ea-91fe-3208b5ae4127.png" alt="rnn4" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

## BPTT(BackPropagation Through Time)

아래는 이렇게 형성된 RNN에 역전파 흐름을 흘려줌으로서 가중치가 학습되는 과정을 그림으로 나타낸 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85720049-02ff8b80-b72b-11ea-8cda-324e82d1ba80.png" alt="rnn5" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

이는 (시간 방향으로) 펼쳐진 신경망에 대해 진행되는 오차역전파법이기 때문에 **BPTT(Backpropagation Through Time)** 이라고 불린다. 하지만 RNN의 크기가 커질수록 역전파 과정에서 필요로하는 컴퓨팅 자원이 기하급수적으로 늘어나기 때문에 이를 적당히 잘라줄(Truncated) 필요가 있다. 커다란 RNN을 여러 개의 작은 RNN으로 자른 뒤 각각의 작은 신경망에서 오차역전파법을 수행한다. 이를 Truncated BPTT라고 하며 아래와 같은 그림으로 나타낼 수 있다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85720128-1a3e7900-b72b-11ea-90b9-3f34d2712b26.png" alt="rnn6" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://cs231n.stanford.edu/syllabus.html">cs231n 강의자료</a></p>

Truncated BPTT에서 중요한 점은 역전파의 흐름만 끊되 순전파의 흐름은 끊지 않는 것이다. 시퀀스 데이터의 연속성(Sequential)을 유지해야 하기 때문이다. 예를 들어, 30개의 RNN계층으로 이루어진 신경망을 10계층씩 3개(0~9, 10~19, 20~29)로 잘라 학습시키는 경우를 생각해보자. 이때 역전파는 토막과 토막 사이를 넘나들지 못하지만 순전파는 그대로 이어나갈 수 있어야 한다. 따라서 2번째 토막의 맨 처음에 입력되는 $h$ 는 이전 토막의 맨 마지막 계층에서 출력된 은닉 상태 벡터 $h_9$ 여야 한다.



## RNN의 문제점

RNN은 여러가지 문제점을 가지고 있다. 그 중 하나가 장기 의존성(Long-Term Dependency)에 대한 문제이다. 다음의 예로 장기 의존성에 대해 알아보자. 

*"Tom was watching TV in his room. Mary came into the room. Mary said hi to (    ?    )."* 이 문장의 빈칸에 들어가야할 단어는 *Tom* 이다. 신경망이 빈칸에 정확히 *Tom*이라는 단어를 집어넣기 위해서는 앞에 나온 문장의 의미를 모두 기억하고 있어야 한다. 하지만 역전파 과정에서 앞에 나온 단어의 기울기는 신경망이 길어질수록 점점 소실되기 때문에 이를 신경망이 간직하지 못하게 된다. 이렇게 기울기가 점점 사라지는 과정을 기울기 소실(Gradient Vanishing)이라고 한다. 이런 문제를 해결하기 위해 RNN에 게이트를 추가한 LSTM, GRU 등의 신경망이 등장하게 된다.