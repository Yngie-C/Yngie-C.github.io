---
layout: post
title: 4. Training Models
category: Hands on Machine Learning
tag: Machine-Learning
---

 

## 1) 선형회귀 

**선형 회귀모델** : 일반적인 선형 회귀 모델은 다음과 같이 나타난다.
$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

- $\hat{y}$ : 예측값, $\quad n$ : 특성의 수,$\quad x_i$ : $i$ 번째 특성값
- $w_j$ : j번째 모델 파라미터 ($w_0$ : 편향,    $w_1, w_2, ... , w_n$ : 특성의 가중치)

위의 식을 벡터 형태로 나타내면
$$
\hat{y} = h_w(\mathbf{x}) =  w^T \cdot{\mathbf{x}}
$$

- $h_w$ : 가설 함수
- $w^T$ : 모델 파라미터 벡터
- $\mathbf{x}$ : 샘플의 특성 벡터

[2장]([https://yngie-c.github.io/hands%20on%20machine%20learning/2020/01/29/homl2/](https://yngie-c.github.io/hands on machine learning/2020/01/29/homl2/)) 에서 회귀모델에 가장 널리 사용되는 지표를 평균 제곱근 오차(RMSE)라고 하였음. 실제로는 RMSE와 같은 결과를 내면서 더 간단한 평균 제곱 오차(Mean Square Error, MSE)를 사용한다.
$$
MSE(\mathbf{X}, h_w) = \frac{1}{m}\sum_{i=1}^m( w^T\cdot\mathbf{x}^{(i)} - y^{(i)})^2
$$


**정규 방정식** : 비용 함수를 최소화 하는 해석적인 방법 (자세한 설명은 [이곳]([https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95#%EC%84%A0%ED%98%95_%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95](https://ko.wikipedia.org/wiki/최소제곱법#선형_최소제곱법)) 을 참조하자)
$$
\hat{w} = (\mathbf{X}^T \cdot \mathbf{X})^{-1} \cdot \mathbf{X}^T \cdot \mathbf{y}
$$


- $\hat{w}$ 는 비용 함수를 최소화하는 $w$ 의 값
- $\mathbf{y}$ 는 $y_1$ 부터 $y_m$ 까지 포함하는 타깃 벡터

정규 방정식을 이용해 $\hat{w}$ 를 구해보자

```python
import numpy as np
X_b = np.c_[np.ones((100, 1)), X]
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

정규 방정식의 계산 복잡도 : $(\mathbf{X}^T \cdot \mathbf{X})^{-1}$ 을 구하기 위한 계산 복잡도(Computational Complexity)는 샘플의 수 $m$ 에 대해서는 $O(m)$ , 특성 수 $n$ 에 대해서는 일반적으로 $O(n^{2.4})$ 에서 $O(n^{3})$ 사이를 나타낸다. 즉, 특성 수가 늘어날수록 정규 방정식을 이용한 방법은 계산 시간이 훨씬 더 오래 걸리게 된다.

<br/>

## 2) 경사 하강법

**경사 하강법** (Gradient Descent, GD) : 매우 일반적인 최적화 알고리즘. 비용 함수를 최소화하는 방향으로 반복해서 파라미터를 조정해 나간다. 구체적으로 보면 $w$ 를 임의의 값으로 시작하여 최솟값에 수렴할 때까지 점진적으로 향상시켜나감. 중요한 파라미터는 스텝의 크기. 이는 **학습률** (Learning rate) 하이퍼 파라미터로 결정한다. 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸리게 되고, 학습률이 너무 크면 적절한 수렴점을 찾지 못할 수도 있다. 

무작위로 결정되는 시작점이 유발하는 경사 하강법의 2가지 문제점

- 지역 최솟값(local minimum)으로의 수렴 : 전역 최솟값(global minimum)이 아닌 지역 최솟값에 수렴할 수도 있다. 
- 평탄한 구간에서의 시간 지연 : 평탄한 구간을 지나게 되는 경우 시간이 오래 걸리고 모델이 일찍 멈추게 된다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택하여 선을 긋더라도 곡선을 가르지 않는 볼록 함수(Convex Function). 오직 하나의 전역 최솟값만을 가진다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않음. [^1] 

특성의 스케일이 다른 경우에도 수렴하는데 시간이 오래 걸릴 수 있으므로 특성 스케일링을 통해 같은 스케일을 갖도록 만들어야 한다.

**배치 경사 하강법** : 경사 하강법을 구현하려면 모델의 각 파라미터 $w_j$ 에 대해서 비용 함수의 그래디언트를 계산. 이를 위해서는 **편도함수** (Partial Derivative)를 구해야 한다. 파라미터 $w_j$ 에 대한 비용 함수의 편도함수는 다음과 같다.
$$
\frac{\partial}{\partial w_j}MSE(w) = \frac{2}{m}\sum_{i=1}^{m}(w^T \cdot \mathbf{x}^{(i)} - y^{(i)})x^{(i)}_j
$$


편도함수는 아래의 식을 사용해 한꺼번에 계산할 수도 있다. [^2]
$$
\nabla_w MSE(w) = \left[\begin{array}{ccc} \frac{\partial}{\partial w_0}MSE(w) \\ \frac{\partial}{\partial w_1}MSE(w) \\ ... \\ \frac{\partial}{\partial w_n}MSE(w) \end{array}\right] = \frac{2}{m}\mathbf{X}^T \cdot (\mathbf{X} \cdot w - \mathbf{y})
$$


그래디언트 벡터가 구해지면 학습률($\eta$) 을 곱하여 기존의 $w$ 에서 빼주어 새로운 $w$ 를 구하는 방법을 계속 반복한다.
$$
w^{d+1} = w^d - \eta \nabla_w MSE(w)
$$


여기서 그래디언트 벡터에 곱해줄 적절한 학습률을 찾기 위해 그리드 탐색을 사용한다. 그리드 탐색에서 수렴하는데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 한다. 반복 횟수가 너무 작으면  최적점에 도달하기 전에 알고리즘이 멈추고, 너무 크면 모델 파라미터가 변하지 않는 동안 시간을 낭비하게 된다.  그래서 일반적으로 반복 횟수는 크게 하되 그래디언트 벡터가 특정 값($\epsilon$)[^3]보다 작아지면 알고리즘을 중지하는 방법을 사용한다.

**확률적 경사 하강법** (Stochastic Gradient Descent) : 배치 경사 하강법은 매 스텝마다 전체 훈련 세트를 이용하여 그래디언트를 계산하므로 훈련 세트가 커지면 시간이 오래 걸린다는 단점이 있다. 반대로 확률적 경사 하강법은 매 스텝에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산한다. 반복마다 사용되는 데이터의 양이 적기 때문에 알고리즘이 훨씬 빠르다.

대신 배치 경사 하강법에 비해 훨씬 불안정하며, 알고리즘이 최솟값과 유사한 값에 도달하긴 하지만 최솟값에 도달하지 못한다. 이런 문제를 해결하기 위해 시작할 때는 학습률을 크게 하고 점차 작게 줄여서 전역 최솟값에 도달하게 하는 학습 스케줄(Learning Schedule) 함수를 사용한다. 일반적으로 한 반복에서 샘플의 개수($m$)만큼 되풀이 되고, 이때의 각 반복을 **에포크** (epoch)라고 한다. 사이킷런에서는 확률적 경사하강법을 위한 SGDRegressor() 클래스를 제공한다.

```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=50, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
```

**미니배치 경사 하강법** (Mini-batch Gradient Descent) : 각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그래디언트를 계산하는 것이 아니라 미니배치라고 부르는 임의의 작은 샘플 세트에 대해 그래디언트를 계산하는 방법.

| 알고리즘             | 샘플 수(m)<br>이 클 때 | 특성 수(n)<br>이 클 때 | 외부 메모리<br>학습 지원 | 스케일<br>조정 필요 | 하이퍼<br>파라미터 수 | 사이킷런<br>클래스 |
| -------------------- | ---------------------- | ---------------------- | ------------------------ | ------------------- | --------------------- | ------------------ |
| 정규방정식           | 빠름                   | 느림                   | X                        | X                   | 0                     | LinearRegression   |
| 배치 경사 하강법     | 느림                   | 빠름                   | X                        | O                   | 2                     | N/A                |
| 확률적 경사 하강법   | 빠름                   | 빠름                   | O                        | O                   | >=2                   | SGDRegressor       |
| 미니배치 경사 하강법 | 빠름                   | 빠름                   | O                        | O                   | >=2                   | N/A                |

<br/>

## 3) 다항 회귀

직선보다 복잡한 데이터셋의 경우. 비선형 데이터를 학습하는 데 선형 모델을 사용할 수 있음. 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키기 때문. 이것을 **다항 회귀** (Polynomial Regression)라고 함. 사이킷런에서는 거듭제곱 특성 생성을 위해 PolynomialFeatures() 라는 클래스를 제공한다.[^4]

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = ploy_features.fit_transform(X)
# degree로 모델의 차수를 정하고 fit_transform()을 이용하여 특성을 생성해준다.
```

<br/>

## 4) 학습 곡선

degree를 많이 높여 고차 다항 회귀를 적용하면 선형 회귀에서보다 훨씬 더 훈련 데이터에 잘 맞추려 한다. 이런 현상은 과적합을 유발한다. 모델이 과적합인지 과소적합인지 알 수 있는 첫 번째 방법은 교차 검증이다. 훈련 데이터에서 모델의 성능이 좋지만 교차 검증 점수가 낮다면 그 모델은 과적합, 둘 모두의 점수가 낮다면 그 모델은 과소적합 되었다고 말합니다.

다음 방법은 **학습 곡선** 을 이용하는 것. 이 곡선은 훈련 세트와 검증 세트의 모델 성능을 훈련 세트 크기의 함수로 나타낸다. 곡선을 그리기 위해서는 다른 크기의 훈련 세트의 서브셋을 만들어 여러 번 훈련시키면 된다. 곡선을 정의하는 함수는 다음과 같다.

```python
from sklearn.metrics import mean_squared_error
from sklearn_model_selection import train_test_split
import matplotlib.pyplot as plt

def plot_learning_curves(model, X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    train_errors, val_errors = [], []
    
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))
    
    plt.plot(np.sqrt(train_errors), label="train_set")
    plt.plot(np.sqrt(val_errors), label="val_set")
```

- 과소적합 학습 모델의 학습 곡선 그래프(아래 그래프는 단순 선형 회귀 모델)

<img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_04in04.png" alt="Underfitting" style="zoom: 33%;" />

먼저 Training set의 그래프(빨간색)를 보면, 훈련 세트에 샘플이 한 개 혹은 두 개일 때는 모델이 완벽하게 작동한다. 그러나 샘플이 점점 추가되면서 모델이 훈련 데이터를 완벽히 학습하는 것이 불가능해진다. 그래서 오차가 일정 수준까지 계속 상승하게 된다. Validation set의 그래프(파란색)를 보면, 샘플이 적을 때는 제대로 일반화가 되지 않아 오차가 크지만, 샘플이 추가될수록 오차 감소가 완만해져서 훈련 세트와 그래프가 가까워진다. 

- 과대적합 학습 모델의 학습 곡선 그래프(아래 그래프는 10차 다항 회귀 모델)

<img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_04in05.png" alt="Overfitting" style="zoom:33%;" />

먼저 Training set의 그래프(빨간색)를 보면, 10차원 모델이므로 샘플 수가 10개 일 때까지는 오차가 없다가 이후부터 오차가 점점 증가하기 시작한다. Validation set의 그래프(파란색)를 보면, 처음에는 모든 데이터에 맞춰진 과적합때문에 오차가 매우 크지만 샘플 수가 많아질수록 오차가 낮아지게 된다.[^5]

- 특징
  - 샘플이 많을 때, 오차의 수준은 과소적합 모델(약 1.8)보다 과대적합 모델(약 0.8~1.1)이 더 낮다.
  - 두 곡선 사이에는 존재하는 공간(즉, 훈련 세트와 검증 세트의 오차 차이)은 과대적합 모델이 더 크다. 이는 훈련 세트의 오차가 검증 데이터보다 훨씬 더 낮다는 뜻인데, 이는 과대적합 모델의 특징이다.

※ **편향 / 분산 트레이드 오프** : 통상적으로 복잡도가 증가할수록, 모델의 분산이 늘어나고 편향은 줄어드는 현상.

- 편향(Bias) : 잘못된 가정으로 인한 오차. 편향이 큰 모델은 훈련 데이터에 과소적합 되기 쉽다.
- 분산(Variance) : 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감할 때 나타나는 오차. 분산이 큰 모델은 훈련 데이터에 과소적합 되기 쉽다.
- 줄일 수 없는 오차 : 데이터 자체의 노이즈로 인한 오차. 이 오차는 데이터에서 노이즈를 제거해야만 없앨 수 있다.

<br/>

## 5) 규제가 있는 선형 모델

과대적합을 감소시키기 위한 규제 방법이 존재한다. 선형 회귀 모델에서는 보통 모델의 가중치를 제한함으로써 규제를 가한다.

**릿지 회귀** (Ridge) : 티호노프(Tikhonov) 규제라고도 부른다. 규제항 $\alpha \sum_{i=1}^n w_1^2$ 이 비용함수에 추가된다. 학습 알고리즘을 데이터에 맞추는 것뿐만 아니라 모델의 가중치가 가능한 한 작게 유지되도록 한다. 규제항은 훈련하는 동안에만 비용함수에 추가되며, 훈련이 끝나면 모델의 성능을 규제가 없는 성능 지표로 평가한다.

$\alpha = 0$ 이면 선형 회귀와 같고, $\alpha$ 가 커질수록 모든 가중치가 0에 가까워지면서 데이터의 평균을 지나는 수평선이 된다. 릿지 회귀의 비용 함수 $f(w)$ 는 아래와 같다.
$$
f(w) = MSE(w) + \alpha \frac{1}{2} \sum_{i=1}^n w_i^2
$$
식에서 볼 수 있듯, $w_0$ 은 규제되지 않으며, $\mathbf{w}^\prime$ 를 $w_1, w_2, ... , w_n$ 라고 정의하면 규제항은 $\frac{1}{2}(\Vert\mathbf{w}^\prime\Vert_2)^2$ 와 같다. 여기서 $\Vert \cdot \Vert_2$ 가 가중체 벡터의 $l_2$ 노름이다. 경사 하강법에 적용하려면 MSE 그래디언트 벡터 $(\nabla_w MSE(w))$ 에 $\alpha \mathbf{w}^\prime$ 을 더하면 된다.

리지 회귀는 정규 방정식과 경사 하강법에 모두 사용 가능하며 릿지 회귀의 정규방정식은 $\hat{w} = (\mathbf{X}^T \cdot \mathbf{X} + \alpha \mathbf{A})^{-1} \cdot \mathbf{X}^T \cdot \mathbf{y}$  이다. 두 방법의 장단점은 선형 회귀와 같다. 사이킷런에서는 릿지 회귀가 적용된 각각의 방법을 다음의 코드로 작동시킬 수 있다.

```python
# 정규 방정식
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=1, solver="cholesky")
ridge_reg.fit(X, y)

# 확률적 경사 하강법
sgd_reg = SGDRegressor(max_iter=5, penalty="12")
sgd_reg.fit(X, y.ravel())
# penalty는 사용할 규제를 지정한다. "12"는 SGD의 비용 함수에 L2 노름의 제곱을 2로 나눈 규제항을 추가하게 한다. (즉, 릿지 회귀와 같다.)
```

**라쏘 회귀** (Lasso) : 라쏘 회귀는 비용 함수에 규제항을 더하지만 $l_2$ 노름의 제곱을 2로 나눈 것 대신 가중치 벡터의 $l_1$ 노름을 사용한다. 라쏘 회귀의 비용 함수 $J(w)$ 는 다음과 같다.
$$
J(w) = MSE(w) + \alpha \sum_{i=1}^n|w_i|
$$
라쏘 회귀의 특징은 덜 중요한 특성의 가중치를 완전히 제거하려고 한다는 점이다. 다시 말해 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델(Sparse model)[^6]을 만든다. 라쏘의 비용 함수는 $w_i = 0$ 에서 미분 가능하지 않다. 하지만 $w_i = 0$ 에서 서브 그래디언트 벡터(Subgradient vector) $\mathbf{g}$ 를 사용하면 경사 하강법을 적용하는 데 문제가 없다.
$$
g(w,J) = \nabla_w MSE(w) + \left[\begin{array}{ccc} sign(w_1) \\ sign(w_2) \\ ... \\ sign(w_n) \end{array}\right] \quad 여기서 \quad sign(w_i) = \begin{cases} -1 \quad w_i < 0 \\ 0 \quad w_i = 0 \\ +1 \quad w_i > 0 \end{cases}
$$
사이킷런에서는 다음과 같은 코드로 라쏘 모델을 작동할 수 있다.

```python
# 정규 방정식
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)

# 확률적 경사 하강법
sgd_reg = SGDRegressor(max_iter=5, penalty="11")
sgd_reg.fit(X, y.ravel())
```



**엘라스틱 넷** (Elastic Net) :  

<br/>

## 6) 로지스틱 회귀



[^1]: 이 함수의 도함수가 립시츠 연속(Lipschitz continuous). 립시츠 연속이란 어떤 함수의 도함수가 일정한 범위 안에서 변할 때 이 함수를 립시츠 연속 함수라고 한다. MSE는 x가 무한대일 때 기울기가 무한대가 되므로 국부적인(locally) 립시츠 함수. 
[^2]: 이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 훈련한다. 때문에 이 알고리즘을 배치 경사 하강법(Batch Gradient Descent)이라고 한다. 배치 경사 하강법은 모든 데이터 셋을 사용하기 때문에 훈련 세트가 커질수록 매우 느려진다. 하지만 특성의 수에는 민감하지 않으므로 특성의 수가 많을 때는 정규 방정식보다 경사 하강법을 사용한다.
[^3]: 비용 함수의 모양에 따라 달라지겠지만 특정 값($\epsilon$) 의 범위 안에서 최적의 솔루션에 도달하기 위해서는 $O(1/\epsilon)$ 의 반복이 걸릴 수 있다. 즉,  $\epsilon$ 을 줄일수록 반복 횟수는 늘어나게 된다.  

[^4]: PolynomialFeatures(degree=d)는 특성이 n개인 배열을 특성이 $\frac{(n+d)!}{d!n!}$ 개인 배열로 변환한다. 특성 수가 엄청나게 늘어날 수 있으니 주의.
[^5]: 과대적합 모델을 개선하는 한 가지 방법은 검증 오차가 훈련 오차에 근접할 때까지 더 많은 훈련 데이터를 추가하는 것.
[^6]: 희소모델은 성분 중 0이 많은(즉, non-zero 값이 희소한 모델을 가리킨다.) 자세한 설명은 [이곳](https://dp-story.tistory.com/4) 을 참조하면 좋다.

