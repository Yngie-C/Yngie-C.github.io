---
layout: post
title: Topic Modeling
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# 잠재 디리클레 할당 (Latent Dirichlet Assignment)

**잠재 디리클레 할당(Latent Dirichlet Assignment, LDA)** 는 토픽 모델링을 위해 가장 널리 사용되는 기법이다. 잠재 디리클레 할당은 하나의 문서는 여러 개의 주제가 혼합된 형태로 이루어져 있다는 것을 전제로 한다. 조금 더 자세히 말하면 각 주제에 해당하는 단어들이 있고 문서는 단어를 뽑을 때 특정 주제에 해당하는 단어를 반복해서 뽑는 식으로 써진다는 것이다.(실제로 사람이 문서를 작성하는 방법과는 다르다) 이를 역으로 추적한다면 각각의 문서에 몇 개의 주제가 섞여 있는지와 그 주제가 얼마만큼의 비중으로 섞여 있는지를 알아낼 수 있다는 것이 잠재 디리클레 할당이 가정하고 있는 바이다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/84987583-631a8e80-b17b-11ea-870c-01d5d71b5ab9.png" alt="lda_generation" style="zoom:80%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

잠재 디리클레 할당에서 가정하고 있는 세 가지를 다시 살펴보자. 첫 번째는 각각의 주제는 ***'단어의 분포로 이루어져 있다'*** 는 것이다. 두 번째는 ***'각 문서가 Corpus-wide한 주제의 혼합으로 이루어져 있다'*** 는 것이다. 즉 말뭉치 전체에 걸쳐있는 주제가 혼합되어 하나의 문서에 있다는 것을 전제로 한다. 동일한 문서 집합에서 추출한 개별 문서가 가지고 있는 주제의 비중은 변하지 않을 것임을 가정한다. 마지막으로 ***'각각의 단어들은 자신이 속해 있는 하나의 주제로부터 샘플링되어 생성된다'*** 는 것이다. 이것이 문서의 생성 과정에서 잠재 디리클레 할당이 가정하고 있는 것들이다.

하지만 우리가 알 수 있는 것은 각각의 단어, 그리고 각각의 문서 뿐이다. 특정 주제에 해당하는 단어 분포가 어떻게 되는지는 알 수 없으며, 문서마다 주제의 비중이 어떻게 구성되어 있는지도 알 수 없다. 그리고 각각의 위치, 즉 첫 번째 혹은 두 번째 단어를 어디서 뽑아와서 배치해야 되는 지도 알지 못한다. 때문에 우리가 알아야 하는 것은 이 세 가지로 추려볼 수 있다. 첫 번째, **어떤 단어가 각각의 주제에 분포하고 있는가?** 두 번째, **각 문서마다 주제의 비중은 어떻게 이루어져 있는가?** 세 번째, **문서의 각 위치에 해당하는 단어가 어느 토픽으로부터 추출이 되었는가?** 우리는 이 세 가지 질문에 대한 답을 알지 못하므로 일련의 과정, 즉 숨겨진(잠재된) 변수를 추론하는 과정을 통해서 이 세 질문에 대한 답을 찾아나가야 한다.



## LDA의 Graphical Notation

다음의 그래프 표현(Graphical Notation)을 통해서 잠재 디리클레 할당이 각각의 답을 찾아가는 과정에 대해서 알아보자.



<p align="center"><img src="https://user-images.githubusercontent.com/45377884/84988849-ad047400-b17d-11ea-8469-dbaff843f44d.png" alt="lda_graph_note" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>



전체적인 과정은 다음의 세 단계로 이루어진다. 먼저 위에서 이야기 했던 가정을 인코딩하고 결합 확률 분포로 이루어진 확률을 각각 Factorization한다. 그리고 이를 알고리즘으로 연결한다. 위 그래프 표현에서 사각형은 단계별 반복을 나타내며 $D$ 는 문서, $N$ 은 단어, $K$ 는 주제를 나타낸다. 각 노드는 랜덤 변수를 나타내고 있으며 $\alpha, \beta$ 는 하이퍼파라미터로 사용자가 조작해야 하는 값이다. $\phi_k$ 는 각 **주제에 대한 단어의 분포** 를 나타낸다. $\theta_d$ 는 각 **문서마다 어떤 주제가 어떤 비중으로 섞여있는지**를 나타내는 값이다. $z_{d,n}$ 는 $d$ 번째 문서에서 $n$ 번째에 등장하는 **단어는 어떤 주제로부터 추출된 것인가** 에 대한 답이다.

각 노드에서 뻗어나가는 엣지는 의존성을 나타낸다. 먼저 가장 왼쪽의 엣지부터 살펴보자. 가장 왼쪽의 엣지는 디리클레 분포를 따르는 $\alpha$ 로부터 $\theta_d$ , 즉 문서마다 있는 주제의 비중을 할당하게 된다. 이로부터 다시 $z_{d,n}$ 으로 나타나는 문서에 등장하는 단어들이 어느 주제로부터 추출된 것인지를 계산해낸다.

주제와 관련된 반대편의 엣지를 보자. 마찬가지로 디리클레 분포를 따르는 $\beta$ 로부터 각 주제에 대한 단어의 분포인 $\phi_k$ 를 알아낸다. 그리고 이 값과 이전에 계산한 $z$ 를 통해서 우리가 관측할 수 있는 값인 $w$, 즉 그 단어가 그 주제에 속하는 단어 중 어떤 것인지를 결정하게 된다. 잠재 디리클레 할당에서 각 단어는 모두 독립적으로 생성된다고 가정하고 있으며 위 그래프 표현을 수식으로 나타내면 아래와 같이 나타낼 수 있다.


$$
p(\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) = \\
\prod^K_{i=1}p(\phi_i|\beta)\prod^D_{d=1}p(\theta_d|\alpha)\prod^N_{n=1}p(z_{d,n}|\theta_d)p(w_{d,n}|\phi_{1:K},z_{d,n})
$$


위 수식에서 첫 번째 식은 맨 오른쪽 엣지인 $\beta$ 로부터 $\phi$ 를 추정하는 엣지이고 두 번째 식은 $\alpha$ 로부터 문서별 주제 분포 $\theta$ 를 알아내는 것이므로 맨 왼쪽 엣지를 나타낸다. 나머지 엣지들은 세 번째 식에 표현이 된다. 세 번째 식에서 앞부분은 왼쪽에서 2번째 엣지를, 마지막 식은 $w$ 양쪽으로 들어오는 엣지를 나타낸다. 조건부 확률로 나타내어져 있는 식을 좀 더 자세히 풀어 써보도록 하자.

먼저 첫 번째 식은 아래와 같이 좀 더 자세하게 쓸 수 있다. $\Gamma$ 와 같은 것들은 아래 디리클레 분포에서 다룰 것이므로 일단은 넘어가도록 하자. 


$$
p(\phi|\beta) = \prod^K_{k=1} \frac{\Gamma(\beta_{k,\cdot})}{\prod^V_{v=1}\Gamma(\beta_{k,v})} \prod^V_{v=1}\phi_{k,v}^{\beta_{k,v}-1}
$$

두 번째 식을 자세히 쓰면 아래와 같은 식이 도출된다. 


$$
p(\theta|\alpha) = \frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\prod^K_{k=1}\Gamma(\alpha_k)} \prod^K_{k=1}\theta_k^{\alpha_k-1}
$$


세 번째 식에서 앞부분 $p(z \vert \theta)$ 를 자세히 쓰면 다음과 같다.


$$
p(z|\theta) = \prod^D_{d=1}\prod^K_{k=1} \theta_{d,k}^{n_{d,k}}
$$


마지막 부분 $p(w \vert z,\theta)$ 은 다음과 같이 쓸 수 있다.


$$
p(w \vert z,\theta) = \prod^K_{k=1}\prod^V_{v=1} \phi_{k,v}^{n_{\cdot ,d,k}}
$$



## 디리클레 분포(Dirichlet Distribution)

위 식들 중 $\Gamma$ 와 그것이 사용된 식을 이해하기 위해서는 디리클레 분포에 대한 이해가 필요하다. 이항 분포부터 다항 분포, 베타 분포를 거쳐 디리클레 분포까지 가는 과정을 알아보도록 하자.

0과 1, 성공과 실패 등 클래스가 2개인 사건이 일어나는 것들이 있다. 각각의 사건을 독립으로 보고 한 사건이 일어날 확률 분포를 나타낸 것을 **이항 확률 분포(Binomial Distribution)** 라고 한다. 이를 주사위처럼 일어나는 클래스가 3개 이상인 것들로 확장해보자. 이 때 한 사건이 일어날 확률 분포를 **다항 확률 분포(Multinomial Distribution)** 라고 한다.

이 확률 분포들에 대한 확률 분포를 다시 생각해 볼 수 있다. 예를 들어, 동전을 2번 던져 앞, 뒷면이 각각 1번 나왔을 때와 10번 던져 앞, 뒷면이 5번씩 나왔을 때, 그리고 100번을 던져 앞, 뒷면이 각각 50번이 나왔을 경우를 생각해보자. 동전의 각 면이 나올 확률을 1/2로 추정할 수 있는 확률은 모두 다를 것이다. 누구나 첫 번째 경우보다는 두 번째가, 두 번째 경우보다는 마지막이 높다고 추정하기 때문이다. 이렇게 이항 분포에 대한 확률 분포를 나타낸 것을 **베타 분포(Beta Distribution)** 라고 하며 베타분포의 수식은 다음과 같이 나타낼 수 있다.


$$
B(\alpha, \beta) = \int^1_0 t_{\alpha-1}(1-t)^{\beta-1}dt = \frac{\Gamma(\alpha)\Gamma(\alpha)}{\Gamma(\alpha+\beta)} \\
\Gamma(x) = \int^\infty_0 \frac{t^{x-1}dt}{e^t}, (x>0) \quad \\
\text{if} \quad x = n \text{(natural number)} \quad \Gamma(n) = (n-1)!
$$


베타 분포가 이항 분포의 확률 분포를 나타낸 것이었다면 **디리클레 분포(Dirichlet Distribution)** 는 다항 분포의 확률 분포를 나타낸 것이다. 이를 식으로 나타내면 다음과 같이 나타낼 수 있다.


$$
p(P=\{p_i\}|\alpha_i) = \frac{\Gamma(\sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \prod_i p_i^{\alpha_i-1}
$$


디리클레 분포는 중요한 특징을 가지고 있다. 사건이 일어난 상황에서 조건부 확률 분포를 그리더라도 그 확률 분포가 다시 디리클레 분포를 따른다는 것. 이를 식으로 나타내면 아래와 같이 쓸 수 있다. 아래에서 첫 번째 식은 아무 사건이 발생하지 않았을 때의 디리클레 분포 식이며 두 번째 식은 디리클레 분포에서 사건이 k번 발생하는 경우를 첫 번째 수식에 대입하여 나타낸 것이다. 마지막은 k번 발생했을 때의 확률 분포를 두 식을 결합하여 나타낸 것이다. 마지막 세 번째 식도 디리클레 분포식의 형태를 따르는 것을 볼 수 있다.


$$
p(P=\{p_i\}|\alpha_i) = \frac{\Gamma(\sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \prod_i p_i^{\alpha_i-1} \\
p(x_1, \cdots, x_k|n, p_1, \cdots, p_k) = \frac{n!}{\prod_i^k x_i} \prod_i p_i^{x_i} \\
p(\{p_i\}|x_1, \cdots, x_k) = \frac{\Gamma(N + \sum_i \alpha_i)}{\prod_i^k \Gamma(\alpha_i + x_i)} \prod_i p_i^{\alpha_i + x_i -1}
$$

각 $\alpha$ 값에 대한 디리클레 분포 변화를 살펴보자. 아래 그림에서 $\alpha$ 가 **큰 값(100)일 때는 모든 케이스가 비슷한 확률로 추정** 되지만 $\alpha$ 가 **작아질수록 점점 극단적** 으로 분포가 변해가는 것을 볼 수 있다. 

<img src="https://user-images.githubusercontent.com/45377884/85116464-54ee7000-b258-11ea-8dbd-a84d59bb5d99.png" alt="lda_alpha" style="zoom:67%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>



잠재 디리클레 할당은 '특정 문서에는 소수의 주제만이 있을 것이다'라는 전제하에 일을 진행하므로 $\alpha$ 에는 1보다 작은 값을 주고 시작하게 되며, '주제별로 단어가 균일한 비중으로 있을 것임'을 전제로 하기 때문에 1에서 시작하여 더 높은 값으로 나아가게 된다. 이 $\alpha, \beta$ 는 하이퍼파라미터이므로 실행할 때마다 우리가 정해주어야 하는 값이며 컴퓨터가 최적화하는 대상은 아니다. Perplexity를 기준으로 GridSearch를 해주어 찾기도 한다.



## Gibbs Sampling

문서 내 단어 ( $\mathbf{w}$ ) 가 주어졌을 때의 잠재 변수( $\phi, \theta, \mathbf{z}$ )의 사후 확률을 구하는 수식은 다음과 같다.


$$
p(\phi, \theta, \mathbf{z} | \mathbf{w}) = \frac{p(\phi, \theta, \mathbf{z} ,\mathbf{w})}{\int_\phi \int_\theta \sum_\mathbf{z} p(\phi, \theta, \mathbf{z} ,\mathbf{w})}
$$


여기서 분모에 해당하는 부분을 정확한 값으로 계산하는 것이 불가능하다. 때문에 우리는 여러가지 근사 알고리즘을 통해서 그 값을 찾아나가야 한다. 이러한 근사 알고리즘 중 **Gibbs Sampling** 이라는 방법을 사용하도록 하자.

Gibbs Sampling은 결합확률 분포를 근사하기 위한 알고리즘으로 마르코프 체인 몬테카를로(Markov Chain Monte Carlo, MCMC)의 알고리즘 중 하나다. Gibbs Sampling이 동작하는 방식은 다음과 같다.[^1] $\{z_1, \cdots, z_N\}$ 라는 확률 변수를 알고 있다는 상태에서 $z_k$ 만 마스킹한 상태로 나머지 값 $\mathbf{z}_{-k}$ 를 이용하여 $z_k$ 의 값을 찾는다. 그리고 이 과정을 지속적으로 반복한다. 표본의 앞 부분은 초기 상태에 좌지우지되는 경향이 있지만 반복 횟수가 늘어날수록 확률 분포 $p$ 에 기반한 표본을 수집할 수 있다.

Gibbs Sampling에는 여러가지 변형된 형태가 있다. 3개의 변수 $a,b,c$ 만 존재하는 경우를 생각해보자. 가장 일반적인 Gibbs Sampling은 나머지 변수를 고정하고 하나의 변수를 계속 변경해나가는 방식이다. $b,c$ 를 고정한 채 $a$ 를 변경하고, $c,a$를 고정한 채 $a$ 를 변경하고, $a,b$ 를 고정한 채 $c$ 를 변경하는 과정을 계속 반복해나간다. Block Gibbs Sampling은 변수를 하나로 묶는다. $a,b$ 를 하나로 묶는 경우를 생각해보자. 먼저 $c$ 를 고정한 채 $a,b$ 를 변화시킨 후에 $a,b$ 를 고정시킨 채로 $c$ 를 변경시키는 과정을 지속적으로 반복하게 된다. 마지막은 잠재 디리클레 할당에서 사용하는 **Collapsed Gibbs Sampling** 이다. 이 방법에서는 일부 변수를 무시한다. 예를 들어, 3개의 변수 중 2개 $(a,c)$ 만을 선택한 뒤에 $a$ 를 고정한 채 $c$ 를 변경, $c$ 를 고정한 채 $a$ 를 변경하는 과정을 반복한다. 배제된 변수 $b$ 는 샘플링 과정에서 collapsed out 된다.

이렇게 Collapsed Gibbs Sampling을 사용하면 변수의 개수를 줄일 수 있다. 우리는 문서에 존재하는 단어가 어떤 것인지, 그리고 마스킹된 단어를 제외하고는 단어가 어떤 주제에 속하는 지를 알고 있다. 때문에 마스킹한 단어가 어떤 주제에 속하는지에 대한 확률은 아래와 같이 나타낼 수 있다. 이 샘플링 과정에서는 각 단어가 어느 토픽에 속하는 지만을 샘플링하므로 나머지 변수 $\theta, \phi$ 는 무시된다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w})
$$


이 확률은 조건부 확률의 정의에 의해서 각 사건의 결합 확률에 비례하게 되며 결합 확률의 식을 조금 변형시킬 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}) \propto p(z_i=j, \mathbf{z}_{-i}, \mathbf{w}) \qquad \qquad \qquad \qquad \qquad \qquad \\
\qquad \qquad \quad= p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}_{-i}) \\
\qquad \quad \qquad \qquad \qquad = p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) p(z_i=j|\mathbf{z}_{-i}) \qquad \cdots \text{ Eq. 1}
$$


Eq. 1 식에서 앞부분은 Factorization하여 다음과 같이 다시 쓸 수 있다. 아래 적분으로 나타내어진 식에서 확률로 나타내어진 첫 번째 항은 이산적인 것으로 계산이 가능하며 두 번째 항은 디리클레 분포로 $\beta$ 를 활용하여 나타낼 수 있다.


$$
p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) = \int p(w_i|z_i=j, \phi^j)p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i})d\phi^j \\ \qquad \qquad
= \int\phi^j_{w_i} p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i})d\phi^j \\
p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i}) \propto p(\mathbf{w}_{-i}|\phi^j,\mathbf{z}_{-i})p(\phi^j) \sim \text{Dirichlet}(\beta + n^w_{-i, j})
$$


이 식에서 $n^w_{-i, j}$ 은 해당 단어를 제외한 모든 단어에 대해서 $j$ 라는 주제에 몇 번이나 할당되었는가를 나타내는 변수이다. 디리클레 분포에 비례한다는 점을 활용하면 원래의 식을 다음과 같은 식으로 변형할 수 있다.


$$
p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) = \frac{n^{w_i}_{-i,j}+\beta}{n^{(\cdot)}_{-i,j}+V \beta}
$$


이번엔 Eq. 1 의 뒷부분을 Factorization 해보자.


$$
p(z_i=j|\mathbf{z}_{-i}) = \int p(z_i=j|\theta^d)p(\theta^d|\mathbf{z}_{-i})d\theta^d \\
p(\theta^d|\mathbf{z}_{-i}) \propto p(\mathbf{z}_{-i}|\theta^d)p(\theta^d) \sim \text{Dirichlet}(n^d_{-i, j} + \alpha)
$$


이 식에서 $n^d_{-i, j}$ 는 문서 내에 있는 단어 중 해당 단어를 제외한 모든 단어가 j라는 토픽에 몇 번이나 할당되었는가를 나타낸다. 이것도 디리클레 분포에 비례한다는 점을 이용하여 다음과 같이 변형할 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}) = \frac{n^d_{-i, j} + \alpha}{n^d_{-i, \cdot} + K\alpha}
$$


Eq. 1에 있는 두 항을 모두 변형된 식으로 고쳐 쓰면 $p(z_i=j|\mathbf{z}_{-i}, \mathbf{w})$ 를 다음과 같이 나타낼 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}) \propto \frac{n^{w_i}_{-i,j}+\beta}{n^{(\cdot)}_{-i,j}+V \beta} \times \frac{n^d_{-i, j} + \alpha}{n^d_{-i, \cdot} + K\alpha}
$$

> $n^d_{-i,j}$ : 해당 단어를 제외하고 문서 $d$ 에 있으며 주제가 $j$ 로 할당된 단어 수 (Document-Topic Count)
>
> $n^d_{-i, \cdot}$ : 해당 단어를 제외하고 문서 $d$ 에 있는 모든 단어 수 (Document-Topic Sum)
>
> $n^{w_i}_{-i,j}$ : 해당 단어를 제외하고 주제 $j$ 로 할당된 같은 단어 수 (Topic-Term Count)
>
> $n^(\cdot)_{-i,j}$ : 해당 단어를 제외하고 주제 $j$ 로 할당된 모든 단어 수 (Topic-Term Sum)



## 잠재 디리클레 할당의 시각화

아래는 지금까지의 내용을 바탕으로 Gibbs Sampling의 과정을 시각화한 자료이다. 아래는 7개 문서로 이루어진 말뭉치에 대한 각 단어를 단순화하여 나타낸 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85148729-6e5cdf80-b28b-11ea-8983-31574fe14ae3.png" alt="lda_img1" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

첫 번째 과정으로 모든 단어에 임의로 주제를 할당 $(Z_0)$ 한다. 해당 말뭉치는 총 6개의 주제로 이루어져 있다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149303-170b3f00-b28c-11ea-8b01-34418e7ae5f7.png" alt="lda_img2" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 첫 번째 문서에 있는 첫 번째 단어를 마스킹한다.

<img src="https://user-images.githubusercontent.com/45377884/85149496-55086300-b28c-11ea-8741-a5035c32f42c.png" alt="lda_img3" style="zoom:50%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 잠재 디리클레 할당 모델을 사용하여 말뭉치 내에 있는 나머지 모든 단어에 할당된 주제를 기반으로 첫 번째 단어의 주제를 계산하여 할당하게 된다. 현재 모델에서는 주제3으로 할당되었다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149670-85e89800-b28c-11ea-98f9-27315fabe15e.png" alt="lda_img4" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

다음으로는 첫 번째 문서에 있는 두 번째 단어를 마스킹한 채 나머지 모든 단어를 기준으로 두 번째 단어의 주제도 할당한다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149959-e546a800-b28c-11ea-9649-ea708943a725.png" alt="lda_img5" style="zoom:50%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85150007-f5f71e00-b28c-11ea-87b0-d457949fa714.png" alt="lda_img6" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

이 과정을 1000번, 10000번 이상 반복하면 할당되는 주제의 변화가 점점 적어진다. 이 비율이 일정 시점 이하로 내려갔을 때 모델을 중단하게 된다. 모델을 평가하는 기준은 아래에서 한 번 더 언급되어 있다.

이번에는 각 단어에 주제가 할당되는 과정을 시각화 자료로 보기로 하자. *"Etruscan trade price temple market"* 라는 문장을 예시로 하자. 먼저 위에서 살펴본 것처럼 각 단어에 랜덤으로 주제를 할당한다. 여기서는 *(3,2,1,3,1)* 이 되었다. 그리고 말뭉치 전체에 대하여 해당 단어가 어떤 주제에 얼마나 할당되었는지를 구하는 표를 그린다. 그리고 단어를 마스킹 한 뒤 표에서 해당하는 부분을 줄여준다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153522-440e2080-b291-11ea-9cdd-54dc0c3592a3.png" alt="lda_slt_img1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 두 가지 질문에 대한 값을 찾는다. 첫 번째 질문은 '이 문서가 각 주제에 얼마나 맞는지' 이며, 두 번째 질문은 '각 주제가 얼마나 해당 단어와 맞는지' 이다. 첫 번째 질문에 대한 계산값은 아래 그림에서 파란색 막대로 나타난다. 이 때 $\alpha$ 가 크면 세 막대의 길이 차이가 거의 없으며, 작으면 값이 한 쪽으로 쏠리게 된다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153768-8a637f80-b291-11ea-8617-d574f9c63578.png" alt="lda_slt_img2" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

두 번째 질문에 대한 계산값은 빨간색 막대와 같이 나타났다고 하자. 이는 주제마다 할당된 단어의 수에 영향을 받게 되며 $\beta$ 값이 커지면 커질수록 주제마다 균일한 값이 나오게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153808-994a3200-b291-11ea-866b-0512c04c8969.png" alt="lda_slt_img3" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

이후 두 값의 곱(아래 그림에서 보라색 넓이)을 기준으로 샘플링하여 다음 단어의 주제를 할당하게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153949-c5fe4980-b291-11ea-8129-6af25fe645cd.png" alt="lda_slt_img4" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 해당 단어의 주제를 업데이트 해주고 위 단순화한 시각 자료에서 본 것처럼 일정 기준을 만족할 때까지 이 과정을 계속해서 반복하게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85155063-3d80a880-b293-11ea-858b-d73017eb1c02.png" alt="lda_slt_img5" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

## 잠재 디리클레 할당 모델 평가하기

이렇게 생성한 잠재 디리클레 할당 모델을 어떻게 평가할 것인가? 절대적인 평가 기준은 없지만 어느 정도 가늠할 수 있는 수치는 존재한다. 가장 많이 사용되는 것은 Perplexity이다. Perplexity의 식은 아래와 같이 나타나게 된다.


$$
\text{Perplexity(w)} = \exp \{ -\frac{\log(p(w))}{\sum^D_{d=1}\sum^V_{j=1}n^{(jd)}}\} \\
\log(p(w)) = \sum^D_{d=1}\sum^V_{j=1}n^{(jd)} \log[\sum^K_{k=1}\theta_K^d\beta_K^j]
$$





















[^1]: 해당 부분은 [위키백과 : 기브스 표집]([https://ko.wikipedia.org/wiki/%EA%B8%B0%EB%B8%8C%EC%8A%A4_%ED%91%9C%EC%A7%91](https://ko.wikipedia.org/wiki/기브스_표집)) 을 참조하였습니다.