---
layout: post
title: 잠재 디리클레 할당 (Latent Dirichlet Allocation)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# Latent Dirichlet Allocation

**잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)**는 토픽 모델링(Topic modeling)을 위해 가장 널리 사용되는 기법입니다. 잠재 디리클레 할당은 다음과 같은 아이디어로부터 출발하였습니다.

> *"하나의 문서에는 여러 여러 개의 주제가 혼합되어 있다. 기본적으로 이 주제에 해당하는 단어들이 있고, 문서는 특정 주제에 해당하는 단어를 반복해서 뽑으면서 작성된다."*

잠재 디리클레 할당에서는 글의 흐름대로 단어를 선택하지 않고 특정 주제에 해당하는 단어를 확률적으로 반복해서 뽑는데 이는 사람이 문서를 작성하는 방법과 꽤 많이 다르다고 볼 수도 있습니다. 아무튼 잠재 디리클레 할당은 이런 가정에서부터 출발하여 특정 주제의 단어가 그 문서에 얼마나 들어 있는지를 파악합니다. 이로부터 문서가 몇 개의 주제로 이루어져 있으며, 각 주제는 얼마만큼의 비중으로 섞여 있는 지를 알 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/84987583-631a8e80-b17b-11ea-870c-01d5d71b5ab9.png" alt="lda_generation" style="zoom:80%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">github.com/pilsung-kang</a></p>

하지만 표면적으로 알 수 있는 것은 각각의 단어와 문서 뿐입니다. 그렇기 때문에 우리는 잠재 디리클레 할당을 통해 다음의 3가지를 알아내려 합니다. 

> 1. 각 주제를 구성하고 있는 단어는 어떤 것인지?
> 2. 각 문서를 구성하는 주제의 비중은 어떻게 되는지?
> 3. 문서의 각 위치에 해당하는 단어는 어느 토픽으로부터 추출이 된 것인지?



## Graphical Notation

다음의 그래프 표현(Graphical Notation)은 잠재 디리클레 할당이 세 문제에 대한 답을 찾는 과정을 나타내고 있습니다.



<p align="center"><img src="https://user-images.githubusercontent.com/45377884/84988849-ad047400-b17d-11ea-8469-dbaff843f44d.png" alt="lda_graph_note" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">github.com/pilsung-kang</a></p>

위 그래프 표현에서 사각형은 각 화살표 단계를 반복함을 나타냅니다. $D, N, K$는 각각 문서, 단어, 주제를 타나냅니다. 파라미터를 나타내고 있는 각 노드를 자세히 보도록 하겠습니다. $\phi_k$는 각 주제에 대한 단어의 분포를 나타내며 $\theta_d$는 각 문서마다 어떤 비중으로 섞여있는 지를 나타냅니다. $z$ 는 $d$ 번째 문서에서 $n$ 번째에 등장하는 단어 $w_{d,n}$이 어떤 주제로부터 추출된 것인가를 나타내는 파라미터입니다.

양쪽 화살표의 출발이 되는 $\alpha, \beta$는 디리클레 분포에서의 하이퍼파라미터입니다. 왼쪽에서 시작하는 $\alpha$를 보도록 하겠습니다. 이 하이퍼파라미터는 각 문서의 주제 비중을 할당하는 $\theta_d$에 영향을 미칩니다. 이렇게 결정된 $\theta_d$는 문서에 등장하는 각 단어가 어떤 주제로부터 추출된 것인지를 계산한 $z_{d, n}$에 영향을 줍니다.

오른쪽은 주제와 관련된 엣지입니다. 시작하는 하이퍼파라미터 $\beta$는 각 주제에 대한 단어 분포인 $\phi_k$를 결정합니다. 그리고 이 값과 이전에 계산한 $z$를 통해서 우리가 관측할 수 있는 값인 $w_{d,n}$를 결정하게 됩니다. 잠재 디리클레 할당에서는 각 단억 모두 독립적으로 생성됨을 가정하고 있으며 위 그래프 표현을 수식으로는 아래와 같이 나타낼 수 있습니다.



$$
p(\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) =\prod^K_{i=1}p(\phi_i|\beta)\prod^D_{d=1}p(\theta_d|\alpha)\prod^N_{n=1}p(z_{d,n}|\theta_d)p(w_{d,n}|\phi_{1:K},z_{d,n})
$$



등호 오른쪽의 식에서 첫 번째 항은 맨 오른쪽 엣지인 $\beta$로부터 $\phi$를 추정하는 과정을 나타냅니다. 두 번째 항은 $\alpha$로부터 문서별 주제 분포인 $\theta$를 알아내는 과정인 맨 왼쪽 엣지를 나타냅니다. 마지막 항은 나머지 모든 엣지를 표현하고 있습니다. 마지막 항의 앞부분은 왼쪽에서 2번째 엣지를, 뒷부분은 $w$의 양쪽으로 들어오는 엣지를 나타냅니다. 조건부 확률로 나타내져 있는 식을 아래와 같이 풀어 쓸 수 있습니다.

먼저 첫 번째 식은 아래와 같이 좀 더 자세하게 쓸 수 있다. $\Gamma$ 와 같은 것들은 아래 디리클레 분포에서 다룰 것이므로 일단은 넘어가도록 하자. 


$$
p(\phi|\beta) = \prod^K_{k=1} \frac{\Gamma(\beta_{k,\cdot})}{\prod^V_{v=1}\Gamma(\beta_{k,v})} \prod^V_{v=1}\phi_{k,v}^{\beta_{k,v}-1}
$$

두 번째 식을 자세히 쓰면 아래와 같은 식이 도출된다. 


$$
p(\theta|\alpha) = \frac{\Gamma(\sum^K_{k=1}\alpha_k)}{\prod^K_{k=1}\Gamma(\alpha_k)} \prod^K_{k=1}\theta_k^{\alpha_k-1}
$$


세 번째 식에서 앞부분 $p(z \vert \theta)$ 를 자세히 쓰면 다음과 같다.


$$
p(z|\theta) = \prod^D_{d=1}\prod^K_{k=1} \theta_{d,k}^{n_{d,k}}
$$


마지막 부분 $p(w \vert z,\theta)$ 은 다음과 같이 쓸 수 있다.


$$
p(w \vert z,\theta) = \prod^K_{k=1}\prod^V_{v=1} \phi_{k,v}^{n_{\cdot ,d,k}}
$$



## 디리클레 분포(Dirichlet Distribution)

위 식들 중 $\Gamma$ 와 그것이 사용된 식을 이해하기 위해서는 디리클레 분포에 대한 이해가 필요하다. 이항 분포부터 다항 분포, 베타 분포를 거쳐 디리클레 분포까지 가는 과정을 알아보도록 하자.

0과 1, 성공과 실패 등 클래스가 2개인 사건이 일어나는 것들이 있다. 각각의 사건을 독립으로 보고 한 사건이 일어날 확률 분포를 나타낸 것을 **이항 확률 분포(Binomial Distribution)** 라고 한다. 이를 주사위처럼 일어나는 클래스가 3개 이상인 것들로 확장해보자. 이 때 한 사건이 일어날 확률 분포를 **다항 확률 분포(Multinomial Distribution)** 라고 한다.

이 확률 분포들에 대한 확률 분포를 다시 생각해 볼 수 있다. 예를 들어, 동전을 2번 던져 앞, 뒷면이 각각 1번 나왔을 때와 10번 던져 앞, 뒷면이 5번씩 나왔을 때, 그리고 100번을 던져 앞, 뒷면이 각각 50번이 나왔을 경우를 생각해보자. 동전의 각 면이 나올 확률을 1/2로 추정할 수 있는 확률은 모두 다를 것이다. 누구나 첫 번째 경우보다는 두 번째가, 두 번째 경우보다는 마지막이 높다고 추정하기 때문이다. 이렇게 이항 분포에 대한 확률 분포를 나타낸 것을 **베타 분포(Beta Distribution)** 라고 하며 베타분포의 수식은 다음과 같이 나타낼 수 있다.


$$
B(\alpha, \beta) = \int^1_0 t_{\alpha-1}(1-t)^{\beta-1}dt = \frac{\Gamma(\alpha)\Gamma(\alpha)}{\Gamma(\alpha+\beta)} \\
\Gamma(x) = \int^\infty_0 \frac{t^{x-1}dt}{e^t}, (x>0) \quad \\
\text{if} \quad x = n \text{(natural number)} \quad \Gamma(n) = (n-1)!
$$


베타 분포가 이항 분포의 확률 분포를 나타낸 것이었다면 **디리클레 분포(Dirichlet Distribution)** 는 다항 분포의 확률 분포를 나타낸 것이다. 이를 식으로 나타내면 다음과 같이 나타낼 수 있다.


$$
p(P=\{p_i\}|\alpha_i) = \frac{\Gamma(\sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \prod_i p_i^{\alpha_i-1}
$$


디리클레 분포는 중요한 특징을 가지고 있다. 사건이 일어난 상황에서 조건부 확률 분포를 그리더라도 그 확률 분포가 다시 디리클레 분포를 따른다는 것. 이를 식으로 나타내면 아래와 같이 쓸 수 있다. 아래에서 첫 번째 식은 아무 사건이 발생하지 않았을 때의 디리클레 분포 식이며 두 번째 식은 디리클레 분포에서 사건이 k번 발생하는 경우를 첫 번째 수식에 대입하여 나타낸 것이다. 마지막은 k번 발생했을 때의 확률 분포를 두 식을 결합하여 나타낸 것이다. 마지막 세 번째 식도 디리클레 분포식의 형태를 따르는 것을 볼 수 있다.


$$
p(P=\{p_i\}|\alpha_i) = \frac{\Gamma(\sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \prod_i p_i^{\alpha_i-1} \\
p(x_1, \cdots, x_k|n, p_1, \cdots, p_k) = \frac{n!}{\prod_i^k x_i} \prod_i p_i^{x_i} \\
p(\{p_i\}|x_1, \cdots, x_k) = \frac{\Gamma(N + \sum_i \alpha_i)}{\prod_i^k \Gamma(\alpha_i + x_i)} \prod_i p_i^{\alpha_i + x_i -1}
$$

각 $\alpha$ 값에 대한 디리클레 분포 변화를 살펴보자. 아래 그림에서 $\alpha$ 가 **큰 값(100)일 때는 모든 케이스가 비슷한 확률로 추정** 되지만 $\alpha$ 가 **작아질수록 점점 극단적** 으로 분포가 변해가는 것을 볼 수 있다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85116464-54ee7000-b258-11ea-8dbd-a84d59bb5d99.png" alt="lda_alpha" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>



잠재 디리클레 할당은 '특정 문서에는 소수의 주제만이 있을 것이다'라는 전제하에 일을 진행하므로 $\alpha$ 에는 1보다 작은 값을 주고 시작하게 되며, '주제별로 단어가 균일한 비중으로 있을 것임'을 전제로 하기 때문에 1에서 시작하여 더 높은 값으로 나아가게 된다. 이 $\alpha, \beta$ 는 하이퍼파라미터이므로 실행할 때마다 우리가 정해주어야 하는 값이며 컴퓨터가 최적화하는 대상은 아니다. Perplexity를 기준으로 GridSearch를 해주어 찾기도 한다.



## Gibbs Sampling

문서 내 단어 ( $\mathbf{w}$ ) 가 주어졌을 때의 잠재 변수( $\phi, \theta, \mathbf{z}$ )의 사후 확률을 구하는 수식은 다음과 같다.


$$
p(\phi, \theta, \mathbf{z} | \mathbf{w}) = \frac{p(\phi, \theta, \mathbf{z} ,\mathbf{w})}{\int_\phi \int_\theta \sum_\mathbf{z} p(\phi, \theta, \mathbf{z} ,\mathbf{w})}
$$


여기서 분모에 해당하는 부분을 정확한 값으로 계산하는 것이 불가능하다. 때문에 우리는 여러가지 근사 알고리즘을 통해서 그 값을 찾아나가야 한다. 이러한 근사 알고리즘 중 **Gibbs Sampling** 이라는 방법을 사용하도록 하자.

Gibbs Sampling은 결합확률 분포를 근사하기 위한 알고리즘으로 마르코프 체인 몬테카를로(Markov Chain Monte Carlo, MCMC)의 알고리즘 중 하나다. Gibbs Sampling이 동작하는 방식은 다음과 같다.[^1] $\{z_1, \cdots, z_N\}$ 라는 확률 변수를 알고 있다는 상태에서 $z_k$ 만 마스킹한 상태로 나머지 값 $\mathbf{z}_{-k}$ 를 이용하여 $z_k$ 의 값을 찾는다. 그리고 이 과정을 지속적으로 반복한다. 표본의 앞 부분은 초기 상태에 좌지우지되는 경향이 있지만 반복 횟수가 늘어날수록 확률 분포 $p$ 에 기반한 표본을 수집할 수 있다.

Gibbs Sampling에는 여러가지 변형된 형태가 있다. 3개의 변수 $a,b,c$ 만 존재하는 경우를 생각해보자. 가장 일반적인 Gibbs Sampling은 나머지 변수를 고정하고 하나의 변수를 계속 변경해나가는 방식이다. $b,c$ 를 고정한 채 $a$ 를 변경하고, $c,a$를 고정한 채 $a$ 를 변경하고, $a,b$ 를 고정한 채 $c$ 를 변경하는 과정을 계속 반복해나간다. Block Gibbs Sampling은 변수를 하나로 묶는다. $a,b$ 를 하나로 묶는 경우를 생각해보자. 먼저 $c$ 를 고정한 채 $a,b$ 를 변화시킨 후에 $a,b$ 를 고정시킨 채로 $c$ 를 변경시키는 과정을 지속적으로 반복하게 된다. 마지막은 잠재 디리클레 할당에서 사용하는 **Collapsed Gibbs Sampling** 이다. 이 방법에서는 일부 변수를 무시한다. 예를 들어, 3개의 변수 중 2개 $(a,c)$ 만을 선택한 뒤에 $a$ 를 고정한 채 $c$ 를 변경, $c$ 를 고정한 채 $a$ 를 변경하는 과정을 반복한다. 배제된 변수 $b$ 는 샘플링 과정에서 collapsed out 된다.

이렇게 Collapsed Gibbs Sampling을 사용하면 변수의 개수를 줄일 수 있다. 우리는 문서에 존재하는 단어가 어떤 것인지, 그리고 마스킹된 단어를 제외하고는 단어가 어떤 주제에 속하는 지를 알고 있다. 때문에 마스킹한 단어가 어떤 주제에 속하는지에 대한 확률은 아래와 같이 나타낼 수 있다. 이 샘플링 과정에서는 각 단어가 어느 토픽에 속하는 지만을 샘플링하므로 나머지 변수 $\theta, \phi$ 는 무시된다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w})
$$


이 확률은 조건부 확률의 정의에 의해서 각 사건의 결합 확률에 비례하게 되며 결합 확률의 식을 조금 변형시킬 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}) \propto p(z_i=j, \mathbf{z}_{-i}, \mathbf{w}) \qquad \qquad \qquad \qquad \qquad \qquad \\
\qquad \qquad \quad= p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}_{-i}) \\
\qquad \quad \qquad \qquad \qquad = p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) p(z_i=j|\mathbf{z}_{-i}) \qquad \cdots \text{ Eq. 1}
$$


Eq. 1 식에서 앞부분은 Factorization하여 다음과 같이 다시 쓸 수 있다. 아래 적분으로 나타내어진 식에서 확률로 나타내어진 첫 번째 항은 이산적인 것으로 계산이 가능하며 두 번째 항은 디리클레 분포로 $\beta$ 를 활용하여 나타낼 수 있다.


$$
p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) = \int p(w_i|z_i=j, \phi^j)p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i})d\phi^j \\ \qquad \qquad
= \int\phi^j_{w_i} p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i})d\phi^j \\
p(\phi^j|\mathbf{z}_{-i}, \mathbf{w}_{-i}) \propto p(\mathbf{w}_{-i}|\phi^j,\mathbf{z}_{-i})p(\phi^j) \sim \text{Dirichlet}(\beta + n^w_{-i, j})
$$


이 식에서 $n^w_{-i, j}$ 은 해당 단어를 제외한 모든 단어에 대해서 $j$ 라는 주제에 몇 번이나 할당되었는가를 나타내는 변수이다. 디리클레 분포에 비례한다는 점을 활용하면 원래의 식을 다음과 같은 식으로 변형할 수 있다.


$$
p(w_i|z_i=j, \mathbf{z}_{-i}, \mathbf{w}_{-i}) = \frac{n^{w_i}_{-i,j}+\beta}{n^{(\cdot)}_{-i,j}+V \beta}
$$


이번엔 Eq. 1 의 뒷부분을 Factorization 해보자.


$$
p(z_i=j|\mathbf{z}_{-i}) = \int p(z_i=j|\theta^d)p(\theta^d|\mathbf{z}_{-i})d\theta^d \\
p(\theta^d|\mathbf{z}_{-i}) \propto p(\mathbf{z}_{-i}|\theta^d)p(\theta^d) \sim \text{Dirichlet}(n^d_{-i, j} + \alpha)
$$


이 식에서 $n^d_{-i, j}$ 는 문서 내에 있는 단어 중 해당 단어를 제외한 모든 단어가 j라는 토픽에 몇 번이나 할당되었는가를 나타낸다. 이것도 디리클레 분포에 비례한다는 점을 이용하여 다음과 같이 변형할 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}) = \frac{n^d_{-i, j} + \alpha}{n^d_{-i, \cdot} + K\alpha}
$$


Eq. 1에 있는 두 항을 모두 변형된 식으로 고쳐 쓰면 $p(z_i=j \vert \mathbf{z}_{-i}, \mathbf{w})$ 를 다음과 같이 나타낼 수 있다.


$$
p(z_i=j|\mathbf{z}_{-i}, \mathbf{w}) \propto \frac{n^{w_i}_{-i,j}+\beta}{n^{(\cdot)}_{-i,j}+V \beta} \times \frac{n^d_{-i, j} + \alpha}{n^d_{-i, \cdot} + K\alpha}
$$

> $n^d_{-i,j}$ : 해당 단어를 제외하고 문서 $d$ 에 있으며 주제가 $j$ 로 할당된 단어 수 (Document-Topic Count)
>
> $n^d_{-i, \cdot}$ : 해당 단어를 제외하고 문서 $d$ 에 있는 모든 단어 수 (Document-Topic Sum)
>
> $n^{w_i}_{-i,j}$ : 해당 단어를 제외하고 주제 $j$ 로 할당된 같은 단어 수 (Topic-Term Count)
>
> $n^(\cdot)_{-i,j}$ : 해당 단어를 제외하고 주제 $j$ 로 할당된 모든 단어 수 (Topic-Term Sum)



## 잠재 디리클레 할당의 시각화

아래는 지금까지의 내용을 바탕으로 Gibbs Sampling의 과정을 시각화한 자료이다. 아래는 7개 문서로 이루어진 말뭉치에 대한 각 단어를 단순화하여 나타낸 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85148729-6e5cdf80-b28b-11ea-8983-31574fe14ae3.png" alt="lda_img1" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

첫 번째 과정으로 모든 단어에 임의로 주제를 할당 $(Z_0)$ 한다. 해당 말뭉치는 총 6개의 주제로 이루어져 있다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149303-170b3f00-b28c-11ea-8b01-34418e7ae5f7.png" alt="lda_img2" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 첫 번째 문서에 있는 첫 번째 단어를 마스킹한다.

<img src="https://user-images.githubusercontent.com/45377884/85149496-55086300-b28c-11ea-8741-a5035c32f42c.png" alt="lda_img3" style="zoom:50%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 잠재 디리클레 할당 모델을 사용하여 말뭉치 내에 있는 나머지 모든 단어에 할당된 주제를 기반으로 첫 번째 단어의 주제를 계산하여 할당하게 된다. 현재 모델에서는 주제3으로 할당되었다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149670-85e89800-b28c-11ea-98f9-27315fabe15e.png" alt="lda_img4" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

다음으로는 첫 번째 문서에 있는 두 번째 단어를 마스킹한 채 나머지 모든 단어를 기준으로 두 번째 단어의 주제도 할당한다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85149959-e546a800-b28c-11ea-9649-ea708943a725.png" alt="lda_img5" style="zoom:50%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85150007-f5f71e00-b28c-11ea-87b0-d457949fa714.png" alt="lda_img6" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

이 과정을 1000번, 10000번 이상 반복하면 할당되는 주제의 변화가 점점 적어진다. 이 비율이 일정 시점 이하로 내려갔을 때 모델을 중단하게 된다. 모델을 평가하는 기준은 아래에서 한 번 더 언급되어 있다.

이번에는 각 단어에 주제가 할당되는 과정을 시각화 자료로 보기로 하자. *"Etruscan trade price temple market"* 라는 문장을 예시로 하자. 먼저 위에서 살펴본 것처럼 각 단어에 랜덤으로 주제를 할당한다. 여기서는 *(3,2,1,3,1)* 이 되었다. 그리고 말뭉치 전체에 대하여 해당 단어가 어떤 주제에 얼마나 할당되었는지를 구하는 표를 그린다. 그리고 단어를 마스킹 한 뒤 표에서 해당하는 부분을 줄여준다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153522-440e2080-b291-11ea-9cdd-54dc0c3592a3.png" alt="lda_slt_img1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 두 가지 질문에 대한 값을 찾는다. 첫 번째 질문은 '이 문서가 각 주제에 얼마나 맞는지' 이며, 두 번째 질문은 '각 주제가 얼마나 해당 단어와 맞는지' 이다. 첫 번째 질문에 대한 계산값은 아래 그림에서 파란색 막대로 나타난다. 이 때 $\alpha$ 가 크면 세 막대의 길이 차이가 거의 없으며, 작으면 값이 한 쪽으로 쏠리게 된다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153768-8a637f80-b291-11ea-8617-d574f9c63578.png" alt="lda_slt_img2" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

두 번째 질문에 대한 계산값은 빨간색 막대와 같이 나타났다고 하자. 이는 주제마다 할당된 단어의 수에 영향을 받게 되며 $\beta$ 값이 커지면 커질수록 주제마다 균일한 값이 나오게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153808-994a3200-b291-11ea-866b-0512c04c8969.png" alt="lda_slt_img3" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

이후 두 값의 곱(아래 그림에서 보라색 넓이)을 기준으로 샘플링하여 다음 단어의 주제를 할당하게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85153949-c5fe4980-b291-11ea-8129-6af25fe645cd.png" alt="lda_slt_img4" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

그리고 해당 단어의 주제를 업데이트 해주고 위 단순화한 시각 자료에서 본 것처럼 일정 기준을 만족할 때까지 이 과정을 계속해서 반복하게 된다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/85155063-3d80a880-b293-11ea-858b-d73017eb1c02.png" alt="lda_slt_img5" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>























[^1]: 해당 부분은 [위키백과 : 기브스 표집]([https://ko.wikipedia.org/wiki/%EA%B8%B0%EB%B8%8C%EC%8A%A4_%ED%91%9C%EC%A7%91](https://ko.wikipedia.org/wiki/기브스_표집)) 을 참조하였습니다.