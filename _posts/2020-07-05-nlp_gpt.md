---
layout: post
title: GPT, GPT-2 (Generative Pre-Training of a language model)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# GPT

<p align="center"><img src="https://openai.com/content/images/2019/05/openai-cover.png" alt="openai" style="zoom: 25%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://openai.com/">openai.com</a></p>

**GPT(Generative Pre-Training of a Language Model)**는 2018년 6월 OpenAI에서 [*"Improving Language Understanding by Generative Pre-Training"*](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 논문을 통해서 발표한 모델입니다. GPT는 시간적으로는 [ELMo](https://yngie-c.github.io/nlp/2020/07/03/nlp_elmo/)이후에 [BERT](https://yngie-c.github.io/nlp/2020/07/04/nlp_bert/)보다는 전에 발표되었습니다. GPT의 기본이 되는 아이디어는 좋은 임베딩 표현을 강조했던 ELMo와 유사하다고 할 수 있습니다.

먼저 특정한 목적 태스크가 없는 대량의 자연어 데이터셋(Unlabeled corpora)을 언어 모델에 학습시킵니다. 이 과정을 Pre-training이라고 합니다. Pre-training을 마친 모델에 추가로 태스크에 맞는 데이터셋(Labeled corpora)을 추가 학습시킵니다. 이를 Fine-tuning이라고 합니다. GPT는 이러한 *"전이 학습(Transfer learning) 방법으로 이전의 모델보다 모든 태스크에서 더 좋은 성능을 낼 수 있을 것이다"*라는 아이디어에서 고안되었습니다.

## Pre-training

GPT는 [트랜스포머(Transformer)](https://yngie-c.github.io/nlp/2020/07/01/nlp_transformer/)의 디코더 블록을 여러 겹 쌓아 만들어 언어 모델(Language models)에 적용하였습니다. 하지만 GPT의 디코더 블록은 트랜스포머의 디코더 블록과 완전히 동일한 구조를 가지고 있지는 않습니다. 트랜스포머에서는 디코더 블록 내에 총 3개의 서브레이어(Sub-layer)를 가지고 있었습니다. 각각 Masked Self-Attention, 인코더-디코더 Attention,  FFNN이었습니다.

하지만 GPT에서는 인코더 블록을 사용하지 않으므로 인코더-디코더 Attention 층이 필요하지 않습니다. 그래서 GPT의 디코더 블록 내부에는 2개의 서브레이어(Masked Self-Attention, FFNN)만 존재하게 됩니다. 아래는 GPT의 구조를 한 눈에 보여주는 이미지입니다.

![gpt_decoder](http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

*"Generative Pre-Training of a Language Model"*이라는 GPT의 이름에서도 알 수 있듯, Pre-training에서 디코더는 언어 모델을 기반으로 작동합니다. 레이블링되지 않은 말뭉치 $U = (u_1, u_2, \cdots ,u_n)$ 에 대하여 다음의 로그 우도를 $L_1$을 최대화 하는 과정으로 진행됩니다. 아래 수식에서 $k$는 윈도우의 사이즈이며, $\Theta$는 학습 파라미터입니다.


$$
L_1(U) = \sum_i \log P(u_i \vert u_{i-k}, \cdots, u_{i-1};\Theta)
$$


학습과정 역시 트랜스포머의 디코더와 유사합니다. 단어 임베딩 $W_e$와 포지셔널 인코딩 $W_p$를 사용하여 첫 번째 디코더에 들어갈 은닉벡터 $h_0$를 마련합니다. 그리고 $n$개의 디코더 블록을 통과시킨 뒤에 최종적으로 나오는 은닉 벡터 $h_n$을 마지막으로 소프트맥스로 통과시켜 각 토큰이 등장할 확률을 구하게 됩니다. 수식으로 나타내면 다음과 같습니다.


$$
\begin{aligned}
h_0 &= UW_e + W_p \\
h_l &= \text{TransformerDecoderBlock}(h_{l-1}), \quad \forall i\in [1,n] \\
P(u) &= \text{Softmax}(h_nW_e^T) 
\end{aligned}
$$


## Fine-Tuning

Fine-tuning에서는 레이블링된 데이터셋 $C = (x_1, \cdots, x_m;y)$를 사용합니다. 각 입력된 데이터는 Pre-trained 모델을 지나면서 레이블 $y$를 예측하는 방향으로 학습을 진행해나가게 됩니다. 로그 우도 $L_2$를 최대화 하는 과정입니다. 수식으로 나타내면 다음과 같습니다.


$$
P(y|x_1, \cdots, x_m) = \text{softmax}(h_l^m W_y) \\
L_2(C) = \sum_{(x,y)} \log P(y|x_1, \cdots, x_m)
$$


논문에서는 Fine-tuning에서 $L_2$뿐만 아니라 $L_1$을 조합하여 사용합니다. 이런 방법이 지도학습 모델의 일반화를 돕고 수렴 속도를 더 빠르게 하는 효과가 있다고 주장하고 있습니다. 아래의 식은 두 목적함수를 결합한 새로운 목적함수 $L_3$를 수식으로 나타낸 것입니다.


$$
L_3(C) = L_2(C) + \lambda \times L_1(C)
$$


GPT의 Fine-tuning은 태스크마다 다른 형태의(Task specific) 데이터셋을 입력해야 합니다. 아래는 태스크에 따라서 다르게 구성되는 데이터셋의 형태를 잘 나타낸 이미지입니다.

<img src="http://jalammar.github.io/images/openai-input%20transformations.png" alt="gpt_input_data"  />

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

먼저 분류(Classification)의 경우에는 별도의 구분자(`Delim`) 없이 텍스트 데이터를 넣습니다. 함의(Entailment)와 관련된 태스크는 전제(Premise)와 가설(Hypothesis)를 구분자로 나누어 입력합니다. 두 텍스트 간의 유사도(Similarity)를 파악하는 태스크는 두 텍스트를 구분자로 나누어 입력하되, 순서를 바꾼 데이터셋을 하나 더 구성하여 함께 입력하게 됩니다. 마지막으로 객관식(Multiple choice)와 관련된 태스크의 경우에는 N개 만큼의 세트를 만듭니다. 각 세트는 구분자를 기준으로 앞쪽에는 동일한 컨텍스트가 위치하고 뒤쪽은 각각의 Answer가 위치합니다. 모든 세트를 각각 다른 트랜스포머에 넣은 뒤 출력되는 벡터에 소프트맥스를 취해주어 최종 답안을 구해내게 됩니다.



## Evalution

논문에서는 아래와 같은 지도학습 데이터셋을 사용하여 이전의 모델과 비교하였습니다. 대부분의 태스크에서 SOTA를 달성한 것을 볼 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521567-a9673180-be8d-11ea-800e-5875ba786711.png" alt="gpt_perf1" style="zoom:67%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521569-ab30f500-be8d-11ea-87c3-ce919c676f15.png" alt="gpt_perf2" style="zoom:67%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521572-ac622200-be8d-11ea-8a9e-e5a21ccfb0bf.png" alt="gpt_perf3" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>

##  Ablation Study

아래 왼쪽은 디코더 블록의 개수와 성능의 관계를 보여주는 그래프입니다. 디코더 블록이 많을수록 성능이 좋아지는 것을 알 수 있습니다.

오른쪽은 각각의 태스크에 대하여 Pre-training 업데이트 횟수와 성능의 관계를 나타낸 그래프입니다. 그래프에서 실선은 트랜스포머를 사용한 경우이며 점선은 LSTM을 사용했을 때의 그래프입니다. 모든 태스크에서 트랜스포머가 LSTM보다는 좋은 성능을 보이는 것을 알 수 있습니다. 그리고 대부분의 태스크에서 업데이트 횟수가 늘어날수록 성능이 늘어나는 경향을 보여주고 있습니다.

![gpt_perf4](https://user-images.githubusercontent.com/45377884/86521599-0531ba80-be8e-11ea-8a9a-8390aa0f8406.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>

아래는 각 데이터셋마다 목적함수를 어떻게 사용하는 지에 따라서 성능을 보여주는 표입니다. 크기가 작은 데이터셋에 대해서는 보조 목적 함수를 사용했을 때 오히려 성능이 떨어지는 것을 볼 수 있습니다. 반대로 커다란 데이터셋에는 보조 목적 함수를 적용해야 더 좋은 성능을 보이는 것을 보여주고 있습니다.

![gpt_perf5](https://user-images.githubusercontent.com/45377884/86521602-0662e780-be8e-11ea-8457-3d8a167363a2.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>

# GPT-2

OpenAI는 GPT에 이어 2019년 2월 *["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)* 라는 논문을 통해 GPT-2 모델을 발표하였습니다. 구조상으로는 이전에 발표했던 GPT와 별 차이가 없지만 더 많은 데이터를 통해서 Pre-training 되었다는 차이점을 가지고 있습니다. GPT-2의 Pre-training에는 무려 40GB정도의 말뭉치가 사용되었습니다.

모델의 크기에 따라 4개의 GPT-2 모델을 발표하였습니다. GPT-2 SMALL의 경우 파라미터의 개수가 1억 1700만(117M)개로 BERT BASE가 사용했던 110M과 비슷한 개수로 맞추어 발표하였습니다. GPT-2 MEDIUM은 3억 4500만(345M)으로 BERT LARGE의 파라미터 개수인 340M과 비슷한 개수로 맞추었지요. OpenAI는 이보다 더 큰 사이즈의 LARGE와 EXTRA LARGE 모델도 공개하였는데 각각 7억 6200만(762M)개, 15억 4200만(1542M)개의 파라미터를 가지고 있습니다. 아래는 GPT-2의 대략적인 크기를 잘 보여주는 이미지입니다.

![gpt2_models](http://jalammar.github.io/images/gpt2/gpt2-sizes.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

## Structure

GPT-2의 구조는 GPT와 거의 유사합니다. GPT-2 SMALL은 12개의 디코더 블록을 쌓아올린 모델이며 임베딩 벡터의 차원 또한 BERT BASE와 동일한 768차원의 임베딩 벡터를 사용하고 있습니다. GPT-2 MEDIUM은 디코더 24개를 쌓아올렸으며 BERT LARGE와 같은 1024차원의 임베딩 벡터를 사용하였습니다. 나머지 GPT-2 LARGE와 EXTRA LARGE는 각각 36개, 48개의 디코더를 쌓아올렸으며 1280, 1600차원의 임베딩 벡터를 사용하였습니다. 아래는 각 모델마다 사용한 디코더 블록의 개수와 임베딩 벡터의 차원 수를 잘 보여주는 이미지입니다.

![gpt2_size](http://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

GPT-2는 최대 1024개의 토큰을 입력받을 수 있으며 GPT-2는 특정 토큰이 생성될 때 항상 가장 확률이 높은 단어(Top word)만 선택되지 않도록 `top_k` 파라미터를 설정하여 샘플링 범위를 넓힐 수 있습니다. 이 파라미터를 조정하면 단어를 생성할 때 $k$개의 단어 보기 중에서 선택하게 되므로 $k$를 키울수록 모델이 다양한 문장을 생성하게 됩니다.

자기 회귀(Auto-regressive)적인 언어 모델 기반으로 작동하므로 이미 생성된 토큰이 다음 토큰의 생성 확률에 영향을 끼치게 됩니다.

![gpt2_generate](http://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

전체 `vocab` 사이즈의 크기는 50,257개이며 단어 임베딩 사이즈는 모델에 맞게 사용합니다. 포지셔널 인코딩 역시 최대 처리할 수 있는 토큰 사이즈에 맞추어 1024개로 설정되어 있습니다. 이렇게 단어 임베딩과 포지셔널 인코딩 벡터를 더하여 모델에 입력하게 됩니다. 아래는 토큰 임베딩 벡터와 포지셔널 인코딩 벡터가 결합되어 모델에 입력된 후 출력 단어가 생성되는 과정을 나타낸 이미지입니다.

<p align="center"><img src="http://jalammar.github.io/images/gpt2/gpt2-token-embeddings-wte-2.png" alt="gpt2_word_token" style="zoom:50%;" /></p>

<p align="center"><img src="http://jalammar.github.io/images/gpt2/gpt2-positional-encoding.png" alt="gpt2_position" style="zoom: 50%;" /></p>

![gpt2_input](http://jalammar.github.io/images/gpt2/gpt2-input-embedding-positional-encoding-3.png)

![gpt2_output](http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io</a></p>

GPT-2는 단어 토큰을 만들기 위해서 BPE(Byte pair encoding)을 사용합니다. 입력할 수 있는 최대 토큰의 개수는 1024개 이며 동시에 처리할 수 있는 토큰 수는 절반인 512개입니다.

## Ablation Study

아래는 파라미터의 개수, 즉 모델의 사이즈에 따라 성능이 변하는 것을 관측한 그래프입니다. Perplexity(PPL)을 기준으로 측정하였으며 모델 사이즈가 커질수록 성능이 좋아지는 것을 알 수 있습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521763-d4eb1b80-be8f-11ea-9c6a-ed22728734aa.png" alt="gpt2_perf1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>

아래는 지도학습 데이터셋에 대한 GPT-2의 성능을 측정한 것입니다. GPT-2 SMALL 모델만으로도 몇 가지 태스크에서 SOTA를 달성할 수 있었으며 가장 사이즈가 큰 EXTRA LARGE 모델의 경우 한 가지 데이터셋을 제외하고는 SOTA를 달성한 것을 볼 수 있습니다. 

![gpt2_perf2](https://user-images.githubusercontent.com/45377884/86521764-d6b4df00-be8f-11ea-9a3c-4be3c2fb8947.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>

## Post BERT, GPT-2

ELMo, GPT, BERT, GPT-2 이후에도 아래 그림과 같이 수많은 모델이 나오고 있습니다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521766-d7e60c00-be8f-11ea-93e8-c3370a6ad0a0.png" alt="gpt2_post_bert" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text Analytics 강의자료</a></p>

아래 그림을 보면 T-NLG가 발;표된 시점까지 파라미터의 개수, 즉 모델의 사이즈가 기하급수적으로 늘고 있는 것을 볼 수 있습니다. 심지어 GPT-2에 이어 2020년 6월에 OpenAI가 발표한 GPT-3에는 약 1750억(175B)개의 파라미터가 사용되었습니다. 이는 그래프에 있는 T-NLG보다도 10배나 많은 파라미터가 사용된 것으로 자연어처리 모델의 스케일이 얼마나 빠르게 커지고 있는 지를 보여주는 사례라고 할 수 있습니다.

![gpt2_post_bert2](https://user-images.githubusercontent.com/45377884/86521767-d9afcf80-be8f-11ea-9999-478135af5202.png)



<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>