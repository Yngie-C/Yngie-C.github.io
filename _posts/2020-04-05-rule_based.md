---
layout: post
title: Rule Based Learning
category: Machine Learning
tag: Machine-Learning
---

 본 포스트는 [카이스트 문일철 교수님의 강의](https://www.edwith.org/machinelearning1_17/joinLectures/9738) 를 바탕으로 작성하였습니다.



# Rule Based Learning

컴퓨터는 경험 $(E)$ 로부터 배움으로써 특정 Task $(T)$ 를 수행하는데, $(E)$ 가 늘어날수록 측정되는 퍼포먼스 $(P)$ 가 높아진다. 압정 던지기 게임을 예를 들어보자. 이 때 더 많은 경험을 위해 더 많이 압정을 던진다거나 더 정확한 사전지식을 적용하는 것은 머신러닝 프로그램을 더 좋아지게 한다. 



## Rule Based Learning

논의를 쉽게 하기 위해서 **완벽한 세계(Perfect world)** 를 가정해보자. 여기서 가정하는 완벽한 세계에서는 관측 오차(Observation errors)가 없고 일관적이지 않은 관측(Inconsistent observations)도 없다. 그리고 어떤 확률적 요소(Stochastic element)도 없으며 우리가 관측하는 정보가 시스템의 모든 정보임을 가정한다.

| 하늘 상태 |  온도  | 습도 | 바람 | 해수 온도 | 일기 예보 | 물놀이를 나갈까? |
| :-------: | :----: | :--: | :--: | :-------: | :-------: | :--------------: |
|   맑음    | 따듯함 | 보통 | 강함 |  따듯함   |  일정함   |        예        |
|   맑음    | 따듯함 | 높음 | 강함 |  따듯함   |  일정함   |        예        |
|   흐림    |  추움  | 높음 | 강함 |  따듯함   |  가변적   |      아니오      |
|   맑음    | 따듯함 | 높음 | 강함 |  차가움   |  가변적   |        예        |



머신러닝의 목적은 경험, 즉 관측된 데이터로부터 더 좋은 근사 함수를 생산하는 것이다. **PAC Learning** 을 다시 떠올려보자. 머신러닝에 필요한 요소들로는 아래와 같은 것들이 있다.

- **인스턴스(Instance)** $X$ : 특성(Feature, $O$ )과 레이블(Label, $Y$ )로 이루어져 있다. 위 식의 각 행을 인스턴스라고 한다. 그 중 맑음, 따듯함, 보통, 강함, 따듯함, 일정함으로 나타나는 것을 특성이라고 하며 예 로 나타나는 것이 레이블이라고 한다.
- Training set $D$ : 관측된 Instance 들을 모아 놓은 것이다.
- **가설(Hypothesis)** $H$ : X가 Y로 바뀔 수 있는 가능성이 있는 함수이다. 위 데이터 셋에서 가능한 가설을 $h_i$ 로 나타내보자. $h_i \rightarrow$ (맑음, 따듯함, ?, ?, ?, 일정함) $\rightarrow$ 예. 가설에서 ?는 하나로 결정되지 않은 특성들이다. 이렇게 결정되지 않은 가설이 있기 때문에 여러 가설이 가능하다.
- **목적 함수(Target Function)** $c$ : 특성과 레이블간의 알려지지 않은 목적 함수이다. 우리의 목표는 특정한 $c$ 가 되는 $h_i$ 를 찾는 것이다.

<br>
$$
\text{Ex) } x_1 : (맑음, 따듯함, 보통, 강함, 따듯함, 일정함) \\
\qquad x_2 : (맑음, 따듯함, 보통, 약함, 따듯함, 일정함) \\
\qquad x_3 : (맑음, 따듯함, 보통, 강함, 따듯함, 가변적) \\
\Rightarrow h_1 \text{<맑음, ?, ?, ?, 따듯함, ?> } (x_1, x_2, x_3) \\
\quad h_2 \text{<맑음, ?, ?, ?, 따듯함, 일정함> } (x_1, x_2) \\
\quad h_3 \text{<맑음, ?, ?, 강함, 따듯함, ?> } (x_1, x_3)
$$



위의 예시에서 $h_1$ 은 Instance의 범위가 **넓고(General),** $h_2, h_3$ 는 Instance의 범위가 **좁다(Specific).**



## Find-S Algorithm & Version Space

**Find-S Algorithm** 은 아무 특성도 한정되지 않은 Null hypothesis 에서 시작한다. 그리고 positive 레이블을 나타내는 인스턴스 하나씩을 골라 해당하는 특성을 가설과 비교하며 판단해나간다. 이 때 현재의 가설과 특성치들이 같으면 아무런 조치를 취하지 않지만 다른 특성치가 나타나면 새로운 값을 포함을 시켜주며 Specific에서 General로 가설의 범위를 확장해나간다.

<br>
$$
\text{위에서 나타난 x_1, x_2, x_3 를 사용하여 Find-S Algorithm 과정을 해보자} \\
h_0 = (\phi, \phi, \phi, \phi, \phi, \phi) \\
h_1 = (맑음, 따듯함, 보통, 강함, 따듯함, 일정함) \\
h_{1,2} = (맑음, 따듯함, 보통, ?, 따듯함, 일정함) \\
h_{1,2,3} = (맑음, 따듯함, 보통, ?, 따듯함, ?)
$$
하지만 이런 과정에서 가능한 가설의 수가 너무 많기 때문에 특정한 하나를 고를 수 없다는 단점이 있다. Find-S Algorithm을 통해 가능한 가설을 모두 모아놓은 범위를 **Version Space(VS)** 라고 한다. Version Space 에는 General한 경계 $(G)$ 가 있고, Specific한 경계 $(S)$ 가 있다.

<br>
$$
VS_{H,D} = \{h \in H \vert \quad \exists s \in S, \exists g \in G, \quad g \geq h \geq s \}
$$

위에서 보았던 데이터를 다시 보도록 하자.

| 하늘 상태 |  온도  | 습도 | 바람 | 해수 온도 | 일기 예보 | 물놀이를 나갈까? |
| :-------: | :----: | :--: | :--: | :-------: | :-------: | :--------------: |
|   맑음    | 따듯함 | 보통 | 강함 |  따듯함   |  일정함   |        예        |
|   맑음    | 따듯함 | 높음 | 강함 |  따듯함   |  일정함   |        예        |
|   흐림    |  추움  | 높음 | 강함 |  따듯함   |  가변적   |      아니오      |
|   맑음    | 따듯함 | 높음 | 강함 |  차가움   |  가변적   |        예        |

여기서 $G : \{(맑음, ?, ?, ?, ?, ?), (?, 따듯함, ?, ?, ?, ?)\}$ 이고 $S : \{(맑음, 따듯함, ?, 강함, ?, ?)\}$ 이고 그 사이에는 각각 $(맑음, ?, ?, 강함, ?, ?), (맑음, 따듯함, ?, ?, ?, ?), (?, 따듯함, ?, 강함, ?, ?)$ 의 가설이 있을 수 있다. 



## Candidate Elimination Algorithm

**Candidate Elimination Algorithm** 은 가장 Specific한 가설 $S$ 와 가장 General한 가설 $G$ 로부터 시작한다. 데이터셋에 있는 인스턴스 하나하나를 사용하여 사이의 후보(Candidate)들을 줄여나간다.(Elimination) positive한 인스턴스 $x_p$ 가 나오면 $x_p$ 를 만족시킬 수 있을 만큼만 S를 점점 일반화 해나간다. 이 과정에서 $G$ 쪽에서 만족이 안된다면 그 조건은 드랍시킨다. 반대로 negative한 인스턴스 $x_n$ 이 나오면 $x_n$ 를 틀리도록 할 수 있게끔 $G$ 를 점점 특정하게 바꾼다. 이 과정에서도 $S$ 쪽에서 만족을 시킨다면 그 조건을 드랍시킨다. 아래의 예시를 보면서 이해해보자.

| 하늘 상태 |  온도  | 습도 | 바람 | 해수 온도 | 일기 예보 | 물놀이를 나갈까? |
| :-------: | :----: | :--: | :--: | :-------: | :-------: | :--------------: |
|   맑음    | 따듯함 | 보통 | 강함 |  따듯함   |  일정함   |        예        |
|   맑음    | 따듯함 | 높음 | 강함 |  따듯함   |  일정함   |        예        |
|   흐림    |  추움  | 높음 | 강함 |  따듯함   |  가변적   |      아니오      |
|   맑음    | 따듯함 | 높음 | 강함 |  차가움   |  가변적   |        예        |

첫 번째와 두 번째의 인스턴스만 가지고 $S$ 와 $G$ 를 발전시켜보자.

<br>
$$
S0 : \{(\phi, \phi, \phi, \phi, \phi, \phi)\} \\
S1 : \{(맑음, 따듯함, 보통, 강함, 따듯함, 일정함)\} \\
S2 : \{(맑음, 따듯함, ?, 강함, 따듯함, 일정함)\} \\ \text{ } \\
G0, G1, G2 : \{(?, ?, ?, ?, ?, ?)\}
$$

위 과정에서는 positive한 인스턴스만을 가지고 $S, G$ 를 고쳐나간다. 그렇기 때문에 $S$ 만 점점 General 해지고 G는 변함이 없다. 다음에는 세 번째 인스턴스만을 가지고 $S, G$ 를 발전시켜보자.

<br>
$$
S3 : \{(맑음, 따듯함, ?, 강함, 따듯함, 일정함)\} \\
G3 : \{(맑음, ?, ?, ?, ?, ?), (?, 따듯함, ?, ?, ?, ?), (?, ?, ?, ?, ?, 일정함)\}
$$

이 과정에서는 negative한 인스턴스만을 다루기 때문에 $S$ 는 변함이 없고 $G$ 만 Specific 해진다. 이어서 네 번째 인스턴스를 가지고 가설의 범위를 좁혀보자.

<br>
$$
S4 : \{(맑음, 따듯함, ?, 강함, ?, ?)\} \\
G4 : \{(맑음, ?, ?, ?, ?, ?), (?, 따듯함, ?, ?, ?, ?)\}
$$

이 과정에서 $S$ 는 General 해지고 $G$ 중 하나는 사라지게 된다. 마지막 특성이 '가변적' 임에도 레이블은 positive이기 때문이다. 

Candidate Elimination Algorithm을 통해 도출한 가설의 범위는 Find-S Algorithm 보다 조금 줄었다. 하지만 아직도 여러가지 경우의 수가 남아있다. 이 중에 어떤 것을 선택하는 것이 좋을까? 아래와 같이 레이블링이 되지 않은 새로운 인스턴스가 추가되는 경우를 생각해보자. 

| 하늘 상태 |  온도  | 습도 | 바람 | 해수 온도 | 일기 예보 | 물놀이를 나갈까? |
| :-------: | :----: | :--: | :--: | :-------: | :-------: | :--------------: |
|   맑음    | 따듯함 | 보통 | 강함 |  차가움   |  가변적   |        ?         |
|   흐림    |  추움  | 높음 | 약함 |  따듯함   |  일정함   |        ?         |
|   흐림    | 따듯함 | 보통 | 강함 |  따듯함   |  일정함   |        ?         |

위 표에서 첫 번째와 두 번째 인스턴스는 위에서 도출한 Version space 밖에 위치하기 때문에 레이블을 결정할 수 있다. 하지만 세 번째 인스턴스는 $S4, G4$ 사이에 위치하기 때문에 함부로 레이블을 결정할 수 없게 된다.

완벽한 세계라는 가정 하에서는 데이터가 늘어나게 되면 **Candidate Elimination Algorithm** 을 통해 가설은 결국 하나로 수렴하게 된다. 하지만 완벽한 세계를 위한 모든 가정은 현실에서는 이루어지기 어려운 것들이다. 현실에서는 모든 인스턴스에 **소음(Noise)** 이 존재하며 다른 결정 요소가 개입될 수 있다. Candidate Elimination Algorithm 을 사용했을 때 맞는 가설도 현실에서는 지워져버리고 말기 때문에 Candidate Elimination Algorithm이 항상 가설을 수렴시킨다고 할 수 없다.